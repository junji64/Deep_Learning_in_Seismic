{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "announced-pathology",
   "metadata": {},
   "source": [
    "## Setup for deep learning study\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "1. On local computer\n",
    "  * Install conda - https://www.anaconda.com/\n",
    "  * execute Jupyter notebook\n",
    "  * install tensorflow, pytorch, etc.\n",
    "  * (with GPU) install cuda, cudnn, ...\n",
    "  \n",
    "  \n",
    "2. Use https://colab.research.google.com/\n",
    "\n",
    "\n",
    "3. Use https://www.kaggle.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-massachusetts",
   "metadata": {},
   "source": [
    "# Deep learning with Python 2nd Ed.\n",
    "<p style='text-align: right;'> By FRANÇOIS CHOLLET </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-column",
   "metadata": {},
   "source": [
    "## Ch.1 What is deep learning\n",
    "\n",
    "딥 러닝이 지금까지 달성한 것은 무엇입니까?  얼마나 중요한가? \n",
    "우리는 다음으로 어디로 향하고 있습니까? \n",
    "과대 광고를 믿어야 합니까?\n",
    "\n",
    "* 기본 개념에 대한 높은 수준의 정의\n",
    "* 머신러닝 발전과정 타임라인\n",
    "* 딥러닝의 대중화와 미래 잠재력의 핵심 요인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-surgery",
   "metadata": {},
   "source": [
    "### 1.1 Artificial intelligence, machine learning, and deep learning\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-01.png\" width=300><p style='text-align: center;'> Figure 1.1 Artificial intelligence, \n",
    "machine learning, and deep learning</p>\n",
    "\n",
    "#### 1.1.1 Artificial intelligence\n",
    "\n",
    "**상징적(symbolic) AI (1950s ~ 1980s)**\n",
    "\n",
    "* 대부분의 전문가들은 프로그래머가 명시적 데이터베이스에 저장된 지식을 조작하기 위해 충분히 많은 명시적 규칙 세트를 직접 만들면 인간 수준의 인공 지능이 달성될 수 있다고 믿었습니다.\n",
    "\n",
    "* 1980년대의 **전문가 시스템(expert systems)** 붐 동안 최고의 인기를 얻었습니다.\n",
    "\n",
    "* 상징적 AI는 잘 정의되고 논리적인 문제를 해결하는 데 적합하다는 것이 입증되었지만,\n",
    "체스를 두는 것과 같이 이미지 분류, 음성 인식 또는 자연어 번역과 같은 더 복잡하고 모호한 문제를 해결하기 위한 명시적인 규칙을 알아내는 것은 난해한 것으로 판명되었습니다. \n",
    "\n",
    "* 상징적인 AI를 대체하기 위해 새로운 접근 방식이 등장했습니다. 바로 머신 러닝입니다.\n",
    "\n",
    "\n",
    "#### 1.1.2 Machine learning\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-02.png\" width=\"300\"><p style='text-align: center;'>Figure 1.2 Machine learning: \n",
    "a new programming paradigm </p>\n",
    "\n",
    "#### 1.1.3 Learning rules and representations from data\n",
    "\n",
    "머신 러닝을 하려면 세 가지가 필요합니다.\n",
    "\n",
    "* 입력 데이터 포인트\n",
    "* 예상 출력의 예\n",
    "* 알고리즘이 잘 작동하는지 측정하는 방법\n",
    "\n",
    "기계 학습 모델은 입력 데이터를 의미 있는 출력으로 변환합니다. 이 프로세스는 입력 및 출력의 알려진 예에 대한 노출을 통해 \"학습\"됩니다. 따라서 머신러닝과 딥러닝의 핵심 문제는 데이터를 의미 있게 변환하는 것입니다.\n",
    "\n",
    "이를 구체화해 봅시다. 그림 1.3과 같이 x축, y축 및 (x,y) 시스템에서 좌표로 표시되는 일부 점을 고려하십시오.\n",
    "보시다시피 몇 개의 흰색 점과 몇 개의 검은 점이 있습니다. 점의 좌표(x, y)를 가져와 그 점이 검은색인지 흰색인지 출력하는 알고리즘을 개발한다고 가정해 보겠습니다. 이 경우,\n",
    "\n",
    "* 입력은 포인트의 좌표입니다.\n",
    "* 예상되는 출력은 포인트의 색상입니다.\n",
    "* 알고리즘이 제대로 작동하는지 측정하는 방법은 예를 들어 올바르게 분류된 포인트의 비율일 수 있습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-03.png\" width=\"100\"><p style='text-align: center;'>Figure 1.3 Some sample data  </p>\n",
    "\n",
    "여기서 필요한 것은 흰색 점과 검은 점을 깔끔하게 분리하는 데이터의 새로운 표현(representation)입니다. 다른 많은 가능성 중에서 우리가 사용할 수 있는 한 가지 변환은 그림 1.4에 나와 있는 좌표 변경입니다.\n",
    "\n",
    "이 표현을 통해 흑백 분류 문제는 \"검은 점은 x > 0\" 또는 \"흰색 점은 x < 0\"과 같은 간단한 규칙으로 표현될 수 있습니다. 이 간단한 규칙과 결합된 이 새로운 표현은 분류 문제를 깔끔하게 해결합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-04.png\" width=\"450\"> <p style='text-align: center;'>Figure 1.4 Coordinate change\n",
    " </p>\n",
    "\n",
    "이 경우 우리는 인간 지능을 사용하여 데이터에 대한 적절한 표현을 생각해 냈습니다. 이것은 매우 간단한 문제에는 문제가 없지만 필기 숫자의 이미지를 분류하는 작업이라면 동일한 작업을 수행할 수 있습니까? \n",
    "\n",
    "일부 개발 데이터 세트에서 올바르게 분류된 숫자의 백분율을 피드백으로 사용하여 좋은 것을 식별하고 자동으로 생성된 데이터 및 이를 기반으로 한 규칙의 다른 세트를 체계적으로 검색하려고 시도하면 어떻게 될까요? \n",
    "그렇다면 우리는 기계 학습을 수행하는 겁니다. \n",
    "머신 러닝의 맥락에서 **학습(learning)**은 **일부 피드백 신호에 의해 안내되는 일부 데이터의 유용한 표현을 생성하는 데이터 변환에 대한 자동 검색 프로세스**를 설명합니다.\n",
    "\n",
    "이러한 변환은 좌표 변경(2D 좌표 분류 예제에서와 같이)이거나 픽셀의 히스토그램 및 루프 수 세기(숫자 분류 예제에서와 같이)일 수 있지만 **선형 투영, 변환, 비선형 연산**(예: \"x > 0과 같은 모든 점 선택\") 등 입니다.\n",
    "\n",
    "#### 1.1.4 The “deep” in “deep learning”\n",
    "\n",
    "\"딥 러닝\"의 \"딥\"은 접근 방식을 통해 달성한 더 깊은 이해를 의미하는 것이 아닙니다. 오히려, 연속적인 표현 레이어라는 아이디어를 나타냅니다. 데이터 모델에 기여하는 레이어 수를 모델의 깊이라고 합니다.\n",
    "\n",
    "딥 러닝 알고리즘이 학습한 표현은 어떻게 생겼습니까? 여러 계층 깊이의 네트워크(그림 1.5 참조)가 숫자를 인식하기 위해 숫자 이미지를 변환하는 방법을 살펴보겠습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-05.png\" width=\"400\"><p style='text-align: center;'>Figure 1.5 A deep neural network \n",
    "for digit classification </p>\n",
    "\n",
    "그림 1.6에서 볼 수 있듯이 네트워크는 숫자 이미지를 원본 이미지와 점점 더 다르고 최종 결과에 대해 점점 더 많은 정보를 주는 표현으로 변환합니다. 심층 네트워크는 정보가 연속적인 필터를 거치고 점점 더 정제되어 나오는(즉, 일부 작업과 관련하여 유용한) **다단계 정보 증류 프로세스**로 생각할 수 있습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-06.png\" width=\"400\"><p style='text-align: center;'>Figure 1.6 Data representations learned by a digit-classification model </p>\n",
    "\n",
    "따라서 딥 러닝은 기술적으로 **데이터 표현을 학습하는 다단계 방법**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-activation",
   "metadata": {},
   "source": [
    "#### 1.1.5 Understanding how deep learning works, in three figures\n",
    "\n",
    "이제 이 학습이 어떻게 이루어지는지 구체적으로 살펴보겠습니다.\n",
    "레이어가 입력 데이터에 대해 수행하는 작업에 대한 사양은 레이어의 가중치에 저장되며, 이는 본질적으로 숫자입니다. 기술적인 측면에서 우리는 레이어에 의해 구현된 변환이 가중치에 의해 매개변수화된다고 말할 수 있습니다(그림 1.7 참조).\n",
    "(가중치는 때때로 계층의 매개변수라고도 합니다.) 이 맥락에서 학습이란 네트워크가 연결된 대상에 예제 입력을 올바르게 매핑하도록 네트워크의 모든 계층 가중치에 대한 값 집합을 찾는 것을 의미합니다. 하지만 여기에 문제가 있습니다. 심층 신경망에는 수천만 개의 매개변수가 포함될 수 있습니다. 모든 매개변수에 대한 올바른 값을 찾는 것은 어려운 작업처럼 보일 수 있습니다. 특히 한 매개변수의 값을 수정하면 다른 모든 매개변수의 동작에 영향을 미칠 수 있습니다!\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-07.png\" width=\"300\"><p style='text-align: center;'> Figure 1.7 A neural network is \n",
    "parameterized by its weights. </p>\n",
    "<br>\n",
    "\n",
    "신경망의 출력을 제어하려면 이 출력이 예상한 것과 얼마나 멀리 떨어져 있는지 측정할 수 있어야 합니다. 이것은 종종 목적 함수(objective function) 또는 비용 함수(cost function)라고도 하는 네트워크의 손실 함수(loss function)의 일입니다. 손실 함수는 네트워크와 실제 목표(네트워크가 출력하기를 원하는 것)를 예측하고 거리 점수(distance score)를 계산하여 이 특정 예에서 네트워크가 얼마나 잘 수행했는지 캡처합니다(그림 1.8 참조).\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-08.png\" width=\"300\"><p style='text-align: center;'> Figure 1.8 A loss function  measures the quality of the network’s output. </p>\n",
    "<br>\n",
    "딥 러닝의 기본 트릭은 이 점수를 피드백 신호로 사용하여 가중치 값을 약간 조정하는 것입니다. 현재 예제의 손실 점수를 낮추는 방향으로(그림 1.9 참조)\n",
    "이 조정은 딥 러닝의 중심 알고리즘인 Backpropagation 알고리즘을 구현하는 최적화 프로그램의 과업입니다. \n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-09.png\" width=\"300\"><p style='text-align: center;'> Figure 1.9 The loss score is \n",
    "used as a feedback signal to adjust the weights.</p>\n",
    "<br>\n",
    "처음에는 네트워크의 가중치에 임의의 값이 할당되므로 네트워크는 일련의 임의 변환을 구현합니다. 당연히 그 출력은 이상적으로 되어야 하는 것과는 거리가 멀고 그에 따라 손실 점수도 매우 높습니다.\n",
    "그러나 네트워크가 처리하는 모든 예에서 가중치가 올바른 방향으로 약간 조정되고 손실 점수가 감소합니다. 이것은 훈련 루프로, 충분한 횟수(일반적으로 수천 개의 예제에 대해 수십 번 반복)를 반복하여 손실 함수를 최소화하는 가중치 값을 생성합니다. 손실이 최소화된 네트워크는 출력이 목표에 최대한 근접한 네트워크입니다. 즉, 훈련된 네트워크입니다.\n",
    "\n",
    "#### 1.1.6 What deep learning has achieved so far\n",
    "\n",
    "* 인간에 가까운 이미지 분류\n",
    "* 인간에 가까운 음성 듣기\n",
    "* 거의 인간 수준의 필기 읽기\n",
    "* 획기적으로 개선된 기계 번역 (https://translate.google.com/?hl=ko)\n",
    "* 획기적으로 개선된 텍스트로부터 음성 변환 (https://www.microsoft.com/en-us/edge)\n",
    "* Google Assistant 및 Amazon Alexa와 같은 디지털 비서\n",
    "* 인간에 가까운 자율주행\n",
    "* Google, Baidu 또는 Bing에서 사용하는 향상된 광고 타겟팅\n",
    "* 웹에서 향상된 검색 결과\n",
    "* 자연어 질문에 답할 수 있는 능력\n",
    "* 슈퍼휴먼 바둑\n",
    "\n",
    "#### 1.1.7 Don’t believe the short-term hype\n",
    "\n",
    "딥 러닝이 최근 몇 년 동안 놀라운 성과를 거두었지만, 이 분야가 향후 10년 동안 달성할 수 있는 것에 대한 기대치는 가능한 것보다 훨씬 더 높은 경향이 있습니다. 자율주행차와 같이 세상을 변화시키는 일부 애플리케이션은 이미 도달할 수 있지만, 믿을 수 있는 대화 시스템, 임의 언어에 대한 인간 수준 기계 번역, 인간 수준 자연어 이해와 같은 더 많은 애플리케이션은 오랫동안 파악하기 어려운 상태로 남아 있을 가능성이 높습니다. 특히 인간 수준의 일반 지능에 대한 이야기를 너무 진지하게 받아들이면 안 됩니다. 단기에 대한 높은 기대치를 가진 위험은 기술이 제공하지 못함에 따라 연구 투자가 고갈되어 장기적으로 진행 속도를 늦추는 것입니다.\n",
    "\n",
    "#### 1.1.8 The promise of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-framing",
   "metadata": {},
   "source": [
    "### 1.2  Before deep learning: A brief history of machine learning\n",
    "\n",
    "딥 러닝이 항상 작업에 적합한 도구는 아닙니다. 딥 러닝을 적용할 수 있는 데이터가 충분하지 않을 때도 있고, 다른 알고리즘으로 문제를 더 잘 해결할 때도 있습니다. 딥 러닝이 머신 러닝을 처음 접하는 경우 모든 것이 불가능한 상황에 처할 수 있습니다.\n",
    "딥 러닝 망치가 있고 모든 머신 러닝 문제는 못처럼 보이기 시작합니다. 이 함정에 빠지지 않는 유일한 방법은 다른 접근 방식에 익숙해지고 적절할 때 활용하는 것입니다.\n",
    "\n",
    "\n",
    "#### 1.2.1 Probabilistic modeling\n",
    "\n",
    "**Naive Bayes 알고리즘** : 데이터 분석에 통계 원칙을 적용하는 확률적 모델링. 기계 학습의 초기 형태 중 하나이며 오늘날에도 여전히 널리 사용됩니다. Naive Bayes는 입력 데이터의 기능이 모두 독립적이라고 가정하면서 Bayes의 정리를 적용한 일종의 기계 학습 분류기입니다. \n",
    "\n",
    "밀접하게 관련된 모델은 현대 기계 학습의 \"Hello World\"로 간주되는 **로지스틱 회귀(logistic regression: logreg)** 입니다. logreg는 회귀 알고리즘이 아니라 분류 알고리즘입니다.\n",
    "Naive Bayes와 마찬가지로 logreg는 컴퓨팅보다 오래되었지만 단순하고 다재다능한 특성 덕분에 오늘날까지 여전히 유용합니다. 데이터 과학자가 당면한 분류 작업에 대한 느낌을 얻기 위해 데이터 세트에서 시도하는 첫 번째 작업인 경우가 많습니다.\n",
    "\n",
    "#### 1.2.2 Early neural networks\n",
    "\n",
    "신경망의 핵심 아이디어는 이미 1950년대에 장난감 형태로 조사되었지만 실제로 활용되기 시작되는 데 수십 년이 걸렸습니다. 오랫동안 누락된 부분은 대규모 신경망을 훈련하는 효율적인 방법이었습니다. 이것은 1980년대 중반에 여러 사람들이 기울기 하강 최적화를 사용하여 매개변수 연산 체인을 훈련하는 방법인 Backpropagation 알고리즘을 독립적으로 재발견하고 이를 신경망에 적용하기 시작한 1980년대 중반에 변경되었습니다.\n",
    "\n",
    "신경망의 첫 번째 성공적인 실제 적용은 1989년 Yann LeCun이 이전의 컨볼루션 신경망과 역전파 개념을 결합하여 손으로 쓴 숫자를 분류하는 문제에 적용했을 때 Bell Labs에서 나왔습니다. LeNet이라고 하는 결과 네트워크는 1990년대에 미국 우편 서비스에서 우편 봉투의 우편 번호 읽기를 자동화하는 데 사용되었습니다.\n",
    "\n",
    "#### 1.2.3 Kernel methods\n",
    "\n",
    "1990년대에 기계 학습에 대한 새로운 접근 방식이 명성을 얻었고 빠르게 신경망을 망각으로 되돌려 놓았습니다. 바로 커널 방법입니다.\n",
    "커널 방법은 분류 알고리즘의 그룹이며 가장 잘 알려진 것은 SVM(Support Vector Machine)입니다. \n",
    "\n",
    "SVM은 두 클래스를 분리하는 \"결정 경계\"를 찾아 작동하는 분류 알고리즘입니다(그림 1.10 참조). SVM은 다음 두 단계로 이러한 경계를 찾습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-10.png\" width=\"100\"><p style='text-align: center;'> Figure 1.10 A decision boundary.</p>\n",
    "\n",
    "1. 데이터는 결정 경계가 초평면으로 표현될 수 있는 새로운 고차원 표현에 매핑됩니다(데이터가 그림 1.10에서와 같이 2차원인 경우 초평면은 직선이 됩니다).\n",
    "\n",
    "2. 좋은 결정 경계(분리 초평면)는 초평면과 각 클래스에서 가장 가까운 데이터 포인트 사이의 거리를 최대화하려고 하여 계산되며, 이 단계를 마진 최대화라고 합니다. 이렇게 하면 경계가 훈련 데이터 세트 외부의 새 샘플로 잘 일반화될 수 있습니다.\n",
    "\n",
    "\n",
    "#### 1.2.4 Decision trees, random forests, and gradient boosting machines\n",
    "\n",
    "의사결정나무는 입력 데이터 포인트를 분류하거나 입력이 주어지면 출력 값을 예측할 수 있는 순서도와 같은 구조입니다(그림 1.11 참조). 시각화하고 해석하기 쉽습니다. 데이터에서 학습된 의사결정나무는 2000년대에 상당한 연구 관심을 받기 시작했으며 2010년까지 커널 방법보다 선호되는 경우가 많았습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-11.png\" width=\"300\"><p style='text-align: center;'> Figure 1.11 A decision tree: the parameters that are learned are the questions about the data. A question could be, for instance, “Is coefficient 2 in the data greater than 3.5?”</p>\n",
    "\n",
    "특히 Random Forest 알고리즘은 많은 수의 특수 결정 트리를 구축한 다음 출력을 앙상블하는 것을 포함하는 결정 트리 학습에 대한 강력하고 실용적인 방식을 도입했습니다. Random Forest 는 광범위한 문제에 적용할 수 있습니다. 모든 얕은 머신러닝 작업에 대해 거의 항상 두 번째로 좋은 알고리즘이라고 말할 수 있습니다.\n",
    "인기 있는 기계 학습 경쟁 웹사이트 Kaggle(http://kaggle.com)이 2010년에 시작되었을 때 랜덤 포레스트는 플랫폼에서 빠르게 인기를 얻었습니다 - 2014년에 그래디언트 부스팅 머신이 소개될 때 까지는. 랜덤 포레스트와 매우 유사한 그래디언트 부스팅 머신은 약한 예측 모델, 일반적으로 결정 트리의 앙상블을 기반으로 하는 머신 러닝 기술입니다. 이전 모델의 약점을 전문적으로 다루는 새 모델을 반복적으로 훈련하여 모든 기계 학습 모델을 개선하는 방법인 그래디언트 부스팅을 사용합니다. 결정 트리에 적용하면 그래디언트 부스팅 기술을 사용하면 유사한 속성을 가지면서도 대부분의 경우 무작위 포리스트보다 성능이 우수한 모델이 생성됩니다.\n",
    "오늘날 비지각적 데이터를 처리하기 위한 최고의 알고리즘은 아닐지라도 최고의 알고리즘 중 하나일 수 있습니다. 딥 러닝과 함께 Kaggle 대회에서 가장 일반적으로 사용되는 기술 중 하나입니다.\n",
    "\n",
    "#### 1.2.5 Back to neural networks\n",
    "\n",
    "분수령의 순간은 2012년에 Hinton의 그룹이 매년 대규모 이미지 분류 챌린지 ImageNet(ImageNet Large Scale Visual Recognition Challenge, 줄여서 ILSVRC)에 참가하면서 시작되었습니다. ImageNet 챌린지는 당시 140만 이미지에 대한 교육 후 고해상도 컬러 이미지를 1,000개의 다른 범주로 분류하는 것으로 악명 높았습니다. 2011년 컴퓨터 비전에 대한 고전적인 접근 방식을 기반으로 한 우승 모델의 상위 5개 정확도는 74.3%에 불과했습니다. 그런 다음 2012년 Alex Krizhevsky가 이끄는 팀과 Geoffrey Hinton이 조언한 팀은 83.6%라는 상위 5위 안에 드는 정확도를 달성할 수 있었습니다. 이는 상당한 혁신이었습니다. 그 이후로 매년 딥 컨볼루션 신경망이 경쟁을 주도해 왔습니다. 2015년까지 승자는 96.4%의 정확도에 도달했고 ImageNet의 분류 작업은 완전히 해결된 문제로 간주되었습니다.\n",
    "\n",
    "#### 1.2.6 What makes deep learning different\n",
    "\n",
    "딥 러닝이 빠르게 발전한 주된 이유는 많은 문제에 대해 더 나은 성능을 제공했기 때문입니다. 그러나 그것이 유일한 이유는 아닙니다. 딥 러닝은 또한 기계 학습 워크플로에서 가장 중요한 단계인 피처 엔지니어링(feature engineering)을 완전히 자동화하기 때문에 문제 해결을 훨씬 더 쉽게 만듭니다.\n",
    "\n",
    "다음은 딥 러닝이 데이터에서 학습하는 방식의 두 가지 필수 특성이 있습니다.\n",
    "* 이러한 중간 증분 표현이 공동으로 학습된다는 사실과, \n",
    "* 각 레이어는 위 레이어의 표현 요구와 아래 레이어의 요구를 모두 따르도록 업데이트됩니다.\n",
    "이 두 가지 속성은 함께 머신 러닝에 대한 이전 접근 방식보다 딥 러닝을 훨씬 더 성공적으로 만들었습니다.\n",
    "\n",
    "\n",
    "#### 1.2.7 The modern machine learning landscape\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-12.png\" width=\"400\"><p style='text-align: center;'> Figure 1.12 Machine learning tools used by top teams on Kaggle </p>\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-13.png\" width=\"400\"><p style='text-align: center;'>Figure 1.13 Tool usage across the machine learning and data science industry (Source: www.kaggle.com/\n",
    "kaggle-survey-2020) </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-cologne",
   "metadata": {},
   "source": [
    "### 1.3 Why deep learning? Why now?\n",
    "일반적으로 다음 세 가지 기술적 요인이 기계 학습의 발전을 주도하고 있습니다.\n",
    "* 하드웨어\n",
    "* 데이터 세트 및 벤치마크\n",
    "* 알고리즘의 발전\n",
    "\n",
    "#### 1.3.1 Hardware\n",
    "\n",
    "#### 1.3.2 Data\n",
    "\n",
    "#### 1.3.3 Algorithms\n",
    "\n",
    "2009–2010년에는 더 나은 기울기 전파를 허용하는 간단하지만 중요한 알고리즘 개선 사항이 몇 가지 추가되었습니다.\n",
    "* 신경층 활성화 기능 향상\n",
    "* 더 나은 가중치 초기화 방식, 레이어별 사전 훈련으로 시작하여 빠르게 중단됨\n",
    "* RMSProp 및 Adam과 같은 더 나은 최적화 방식\n",
    "\n",
    "#### 1.3.4 A new wave of investment\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/01-14.png\" width=\"400\"><p style='text-align: center;'>Figure 1.14 OECD estimate of total investments in AI startups (Source: http://mng.bz/zGN6)\n",
    "</p>\n",
    "\n",
    "\n",
    "#### 1.3.5 The democratization of deep learning\n",
    "\n",
    "#### 1.3.6 Will it last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-replica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-ferry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
