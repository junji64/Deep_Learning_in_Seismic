{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Ch.11.(2/2) Deep learning for text\n",
    "\n",
    "Chapter contents:\n",
    "\n",
    "* Transformer 아키텍처\n",
    "* Sequence-to-sequence 학습\n",
    "\n",
    "\n",
    "## Ch.11.3 The Transformer architecture\n",
    "\n",
    "2017년부터 새로운 모델 아키텍처: Transformer (Attention is all you need)\n",
    "\n",
    "\"neural attention\"이라는 간단한 메커니즘을 사용하여 순환(recurrent) 레이어나 컨볼루션(convolution) 레이어가 없는 강력한 시퀀스 모델을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Understanding self-attention\n",
    "\n",
    "모델이 보는 모든 입력 정보가 당면한 작업에 똑같이 중요하지는 않습니다.\n",
    "\n",
    "모델은 일부 특징에 \"더 많은 주의를 기울이고(pay more attention)\" 다른 특징에 \"덜 주의를 기울여야(pay less attention)\" 합니다.\n",
    "\n",
    "비슷한 생각:\n",
    "\n",
    "* convnet의 Max pooling은 공간 영역의 특징 풀을 살펴보고, 유지할 특징을 하나만 선택합니다.\n",
    "\n",
    "\n",
    "* TF-IDF 정규화는 다른 토큰이 전달할 가능성이 있는 정보의 양에 따라 토큰에 중요도 점수를 할당합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-05.png\" width=\"300\"><p style=\"text-align:center\">Figure 11.5 The general concept of “attention” in deep learning: input features get assigned “attention scores,” which can be used to inform the next representation of the input. From raw text to vectors.</p>\n",
    "\n",
    "Attention 메커니즘을 사용하여 특징을 문맥-인식(context-aware)이 가능하게 할 수 있습니다.\n",
    "\n",
    "벡터 공간은 서로 다른 단어 간의 의미 관계의 \"모양\"을 캡처합니다.\n",
    "\n",
    "한 단어는 고정된 위치를 갖는다\n",
    "\n",
    "그러나, 단어의 의미는 일반적으로 문맥에 따라 다릅니다.\n",
    "\n",
    "스마트 임베딩 공간: 단어를 둘러싼 다른 단어에 따라 단어에 대해 다른 벡터 표현 제공\n",
    "\n",
    "self-attention의 목적은 시퀀스에서 관련 토큰의 표현을 사용하여 토큰의 표현을 변조하는 것입니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06.png\" width=\"500\"><p style=\"text-align:center\">Figure 11.6. Self-attention: attention scores are computed between “station” and every other word in the sequence, and they are then used to weight a sum of word vectors that becomes the new “station” vector.</p>\n",
    "\n",
    "* Compute relevancy scores (dot product)\n",
    "* Scale (1/sqrt) and softmax\n",
    "* Weighted (scores) sum of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=input_sequence.shape)\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        scores = np.zeros(shape=(len(input_sequence),))\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector, vector.T)\n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = softmax(scores)\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += vector * scores[j]\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Generalized self-attention: the query-key-value model\n",
    "\n",
    "self-attention을 위한 백터화 구현\n",
    "```\n",
    "    num_heads = 4 \n",
    "    embed_dim = 256 \n",
    "    mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    outputs = mha_layer(inputs, inputs, inputs)\n",
    "```\n",
    "지금까지 단 하나의 시퀀스를 사용함. \n",
    "그러나 기계 번역용으로 개발된 Transformer (2개의 시퀀스: source, target)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06-UN01.png\" width=\"450\">\n",
    "\n",
    "* 입력(A)의 각 토큰에 대해, \n",
    "* 토큰이 입력(B)의 모든 토큰과 얼마나 관련되어 있는지 계산\n",
    "* 이 점수를 사용하여 입력(C)의 토큰 합계에 가중치를 부여합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06-UN02.png\" width=\"450\">\n",
    " \n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-07.png\" width=\"500\"><p style=\"text-align:center\">Figure 11.7 Retrieving images from a database: the “query” is compared to a set of “keys,” and the match scores are used to rank “values” (images).</p>\n",
    "\n",
    "* Source: \"How's the weather today?\"\n",
    "* Target: \"¿Qué tiempo hace hoy?\" \n",
    "\n",
    "* query: target\n",
    "* keys: source\n",
    "* values: source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "다중 헤드(Multi-head): self-attention layer의 출력 공간은 개별적으로 학습된 독립적인 부분 공간 세트로 나누어집니다.\n",
    "\n",
    "초기 쿼리(query), 키(key) 및 값(value)은 3개의 독립적인 dense projections 세트를 통해 전송되어 3개의 개별 벡터가 생성됩니다.\n",
    "\n",
    "각 벡터는 neural attention 을 통해 처리됩니다.\n",
    "\n",
    "3개의 출력은 다시 하나의 출력 시퀀스로 연결됩니다.\n",
    "\n",
    "학습 가능한 dense projections의 존재는 레이어가 무언가를 학습할 수 있도록 합니다(순수한 상태없는 변환(stateless transformation)과 반대).\n",
    "\n",
    "독립 헤드(Independent heads): 레이어가 각 토큰에 대한 다양한 특징 그룹을 학습하도록 돕습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-08.png\" width=\"450\"><p style=\"text-align:center\">Figure 11.8 The MultiHeadAttention layer</p>\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"200\"><p style=\"text-align:center\">Multi-head attention (Another perspective)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Transformer encoder\n",
    "\n",
    "Dense projections 작업 -> attention 메커니즘의 출력에도 추가함.\n",
    "\n",
    "또한\n",
    "\n",
    "* residual connection 추가(깊어지기 때문에)\n",
    "* 정규화 추가(역전파 동안 더 나은 gradients 흐름을 위해)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-09.png\" width=\"200\"><p style=\"text-align:center\">Figure 11.9 The TransformerEncoder chains a MultiHeadAttention layer with a dense projection and adds normalization as well as residual connections.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# import os, pathlib, shutil, random\n",
    "# from tensorflow import keras\n",
    "# batch_size = 32\n",
    "# base_dir = pathlib.Path(\"aclImdb\")\n",
    "# val_dir = base_dir / \"val\"\n",
    "# train_dir = base_dir / \"train\"\n",
    "# for category in (\"neg\", \"pos\"):\n",
    "#     os.makedirs(val_dir / category)\n",
    "#     files = os.listdir(train_dir / category)\n",
    "#     random.Random(1337).shuffle(files)\n",
    "#     num_val_samples = int(0.2 * len(files))\n",
    "#     val_files = files[-num_val_samples:]\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(train_dir / category / fname,\n",
    "#                     val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 600), dtype=int64, numpy=\n",
       " array([[  15,  232,   15, ...,    0,    0,    0],\n",
       "        [  89,   27, 4403, ...,    0,    0,    0],\n",
       "        [  44,   23,   67, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  10,   26, 4306, ...,    0,    0,    0],\n",
       "        [ 463,    2, 1651, ...,    0,    0,    0],\n",
       "        [ 356,   17,    8, ...,    0,    0,    0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(int_test_ds.take(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Transformer encoder implemented as a subclassed `Layer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the Transformer encoder for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 256)        543776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and evaluating the Transformer encoder based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 35s 49ms/step - loss: 0.4706 - accuracy: 0.7812 - val_loss: 0.3787 - val_accuracy: 0.8334\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.2997 - accuracy: 0.8753 - val_loss: 0.3208 - val_accuracy: 0.8628\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.2353 - accuracy: 0.9056 - val_loss: 0.4505 - val_accuracy: 0.8248\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1866 - accuracy: 0.9290 - val_loss: 0.3171 - val_accuracy: 0.8792\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1514 - accuracy: 0.9423 - val_loss: 0.3410 - val_accuracy: 0.8914\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1284 - accuracy: 0.9507 - val_loss: 0.3877 - val_accuracy: 0.8832\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.1092 - accuracy: 0.9596 - val_loss: 0.4322 - val_accuracy: 0.8742\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0917 - accuracy: 0.9658 - val_loss: 0.5636 - val_accuracy: 0.8736\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.0794 - accuracy: 0.9707 - val_loss: 0.5835 - val_accuracy: 0.8646\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.0708 - accuracy: 0.9743 - val_loss: 0.5027 - val_accuracy: 0.8676\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.0607 - accuracy: 0.9786 - val_loss: 0.7366 - val_accuracy: 0.8596\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 0.6237 - val_accuracy: 0.8698\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0445 - accuracy: 0.9838 - val_loss: 0.7969 - val_accuracy: 0.8660\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0378 - accuracy: 0.9868 - val_loss: 0.6745 - val_accuracy: 0.8684\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0315 - accuracy: 0.9894 - val_loss: 0.6830 - val_accuracy: 0.8654\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.9104 - val_accuracy: 0.8642\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0203 - accuracy: 0.9932 - val_loss: 1.1169 - val_accuracy: 0.8560\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0182 - accuracy: 0.9938 - val_loss: 0.9610 - val_accuracy: 0.8610\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 1.3211 - val_accuracy: 0.8546\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 0.8345 - val_accuracy: 0.8660\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3477 - accuracy: 0.8626\n",
      "Test acc: 0.863\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872\n",
    "* glove_embeddings_sequence model: 0.868\n",
    "* transformer_encoder: 0.874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 모델보다 좋지 않습니다. 왜요?\n",
    "\n",
    "현재 Transformer 모델: 시퀀스 모델이 아닙니다.\n",
    "\n",
    "Attention 계층: 토큰을 집합(set)으로 봅니다.\n",
    "\n",
    "토큰 순서 변경: 동일한 컨텍스트 인식 표현\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-10.png\" width=\"250\"><p style=\"text-align:center\">Figure 11.10 Features of different types of NLP models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Using positional encoding to re-inject order information\n",
    "\n",
    "\n",
    "위치 인코딩: 모델에 단어-순서 정보에 대한 액세스 부여\n",
    "\n",
    "각 단어 임베딩에 단어의 위치 추가\n",
    "\n",
    "워드 임베딩 구성요소:\n",
    "* 단어 벡터(word vector)(독립 표현)\n",
    "* 위치 벡터(position vector)(단어의 위치를 나타냄)\n",
    "\n",
    "구현\n",
    "\n",
    "모델은 위치-임베딩(position-embedding) 벡터를 학습합니다(단어 인덱스를 포함하는 것과 동일한 방식)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Implementing positional embedding as a subclassed layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Putting it all together: A text-classification Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Combining the Transformer encoder with positional embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        5273600   \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 32s 49ms/step - loss: 0.4742 - accuracy: 0.7831 - val_loss: 0.3356 - val_accuracy: 0.8538\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.2372 - accuracy: 0.9088 - val_loss: 0.3107 - val_accuracy: 0.8882\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.1733 - accuracy: 0.9380 - val_loss: 0.2753 - val_accuracy: 0.8864\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1445 - accuracy: 0.9475 - val_loss: 0.3069 - val_accuracy: 0.8860\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.1238 - accuracy: 0.9560 - val_loss: 0.3794 - val_accuracy: 0.8900\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.1069 - accuracy: 0.9622 - val_loss: 0.3961 - val_accuracy: 0.8834\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.0973 - accuracy: 0.9649 - val_loss: 0.4008 - val_accuracy: 0.8550\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.0859 - accuracy: 0.9703 - val_loss: 0.4321 - val_accuracy: 0.8836\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0745 - accuracy: 0.9736 - val_loss: 0.4572 - val_accuracy: 0.8670\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0690 - accuracy: 0.9774 - val_loss: 0.5206 - val_accuracy: 0.8722\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0606 - accuracy: 0.9798 - val_loss: 0.5073 - val_accuracy: 0.8768\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0539 - accuracy: 0.9815 - val_loss: 0.4345 - val_accuracy: 0.8750\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0504 - accuracy: 0.9837 - val_loss: 0.6034 - val_accuracy: 0.8762\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.0423 - accuracy: 0.9851 - val_loss: 0.4783 - val_accuracy: 0.8748\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0348 - accuracy: 0.9893 - val_loss: 0.7015 - val_accuracy: 0.8724\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0302 - accuracy: 0.9903 - val_loss: 0.6915 - val_accuracy: 0.8700\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.7759 - val_accuracy: 0.8720\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0247 - accuracy: 0.9926 - val_loss: 0.8284 - val_accuracy: 0.8720\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.8020 - val_accuracy: 0.8654\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.8672 - val_accuracy: 0.8690\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.3162 - accuracy: 0.8662\n",
      "Test acc: 0.866\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872\n",
    "* glove_embeddings_sequence model: 0.868\n",
    "* transformer_encoder: 0.874\n",
    "* full_transformer_encoder: 0.882\n",
    "\n",
    "Improved but still not as good as the Bag-of-words ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### When to use sequence models over bag-of-words models?\n",
    "\n",
    "Bag-of-words는 구식이 아닙니다.\n",
    "\n",
    "어떤 경우에는 bag-of-bigrams 위에 Dense layers의 작은 스택이 완벽하게 유효합니다.\n",
    "\n",
    "다양한 유형의 텍스트 데이터 세트에 대한 다양한 텍스트 분류 기술 분석\n",
    "\n",
    "중요 기준: 훈련 데이터의 샘플 수와 샘플당 평균 단어 수 간의 비율\n",
    "\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-11.png\" width=\"350\"><p style=\"text-align:center\">Figure 11.11 A simple heuristic for selecting a text-classification model: the ratio between the number of training samples and the mean number of words per sample</p>\n",
    "\n",
    "시퀀스 모델은 다음과 같은 경우에 가장 잘 작동합니다.\n",
    "\n",
    "* 많은 훈련 데이터를 사용할 수 있으면서\n",
    "* 각 샘플은 상대적으로 짧은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond text classification: Sequence-to-sequence learning\n",
    "\n",
    "시퀀스-투-시퀀스(Sequence-to-sequence) 학습 응용\n",
    "\n",
    "* 기계 번역\n",
    "* 텍스트 요약(summarization)\n",
    "* 질의응답\n",
    "* 챗봇(Chatbot)\n",
    "* 텍스트 생성 \n",
    "* etc.\n",
    "\n",
    "일반 템플릿:\n",
    "\n",
    "  * 인코더: 소스 시퀀스를 중간 표현(intermediate representation)으로 바꿉니다.\n",
    "  * 디코더: 다음의 두개를 보고 목표 시퀀스의 다음 토큰 i를 예측하도록 훈련됨\n",
    "      * 이전 토큰(0 to i-1) 및\n",
    "      * 인코딩된 소스 시퀀스\n",
    "\n",
    "추론 과정: 목표 시퀀스에 액세스할 수 없습니다. 한 번에 하나의 토큰을 생성합니다.\n",
    "\n",
    "1. 인코더에서 인코딩된 소스 시퀀스를 얻습니다.\n",
    "2. 디코더는 다음으로 시작합니다.\n",
    "   * 인코딩된 소스 시퀀스\n",
    "   * 초기 \"시드\" 토큰 [start]\n",
    "  \n",
    "   시퀀스의 첫 번째 실제 토큰을 예측합니다.\n",
    "  \n",
    "3. 디코더로 피드백되는 예측 시퀀스(스톱 토큰 [end] 가 생성될 때까지)\n",
    "\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-12.png\" width=\"450\"><p style=\"text-align:center\">Figure 11.12 Sequence-to-sequence learning: the source sequence is processed by the encoder and is then sent to the decoder. The decoder looks at the target sequence so far and predicts the target sequence offset by one step in the future. During inference, we generate one target token at a time and feed it back into the decoder.</p>\n",
    "\n",
    "<img src=\"https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A machine translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tom waited patiently for Mary.', '[start] Tom esperó pacientemente a Mary. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs   = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs  = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing the English and Spanish text pairs**\n",
    "\n",
    "2개의 TextVectorization 레이어 준비\n",
    "\n",
    "일반적으로 [ 및 ] 문자가 제거됩니다만, 이번 예제에서는 [start] 및 [end]은 보존되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,     # for [start] or [end]\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing datasets for the translation task**\n",
    "\n",
    "튜플(tuple) 반환(inputs, target)\n",
    "\n",
    "* 입력은 \"encoder_inputs\"(영어 문장) 및 \"decoder_inputs\"(스페인어 문장)의 두 키가 있는 dictionary 입니다.\n",
    "\n",
    "* target은 한 단계 앞선 오프셋을 갖는 스페인어 문장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],    # [start], 1, 2. ..., n\n",
    "    }, spa[:, 1:])                 # 1, 2. ..., n, [end]\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence learning with RNNs\n",
    "\n",
    "2015-2017년에는 시퀀스-투-시퀀스 학습에서 RNN이 지배적이었음 (2017년경 Google 번역 참조)\n",
    "\n",
    "시퀀스를 시퀀스로 바꾸는 순진한 방법\n",
    "\n",
    "```\n",
    "inputs = keras.Input(shape=(sequence_length,), dtype='int64')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)         # See this return_sentences\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "```\n",
    "<img src=\"https://miro.medium.com/max/1400/1*zeOG90tmye654fQjLslkQA.png\" width=\"400\" >\n",
    "\n",
    "두 가지 주요 문제:\n",
    "* 타겟 seq 길이는 소스 seq와 같아야 합니다.\n",
    "* 모델은 타겟에서 토큰 N을 예측하기 위해 소스에서 토큰 0...N만 참고합니다.\n",
    "\n",
    "\n",
    "적절한 방법:\n",
    "\n",
    "* 전체 seq를 벡터(들)의 집합으로 인코딩\n",
    "* N+1을 예측하기 위해 target seq에서 위의 것들과 함께 elemens 0...N을 제공합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-13.png\" width=\"350\"><p style=\"text-align:center\">Figure 11.13 A sequence-to-sequence RNN: an RNN encoder is used to produce a vector that encodes the entire source sequence, which is used as the initial state for an RNN decoder.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU-based encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile3.uf.tistory.com%2Fimage%2F99D1914F5B2917F136F0E5\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU-based decoder and the end-to-end model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training our recurrent sequence-to-sequence model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 91s 64ms/step - loss: 1.6376 - accuracy: 0.4166 - val_loss: 1.3168 - val_accuracy: 0.5069\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 1.3157 - accuracy: 0.5255 - val_loss: 1.1546 - val_accuracy: 0.5691\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 1.1735 - accuracy: 0.5757 - val_loss: 1.0726 - val_accuracy: 0.6003\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0828 - accuracy: 0.6076 - val_loss: 1.0386 - val_accuracy: 0.6204\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0358 - accuracy: 0.6326 - val_loss: 1.0209 - val_accuracy: 0.6298\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0050 - accuracy: 0.6507 - val_loss: 1.0166 - val_accuracy: 0.6363\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 85s 65ms/step - loss: 0.9859 - accuracy: 0.6638 - val_loss: 1.0163 - val_accuracy: 0.6391\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 85s 66ms/step - loss: 0.9724 - accuracy: 0.6744 - val_loss: 1.0191 - val_accuracy: 0.6411\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 87s 66ms/step - loss: 0.9622 - accuracy: 0.6824 - val_loss: 1.0221 - val_accuracy: 0.6410\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9546 - accuracy: 0.6880 - val_loss: 1.0235 - val_accuracy: 0.6438\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9491 - accuracy: 0.6926 - val_loss: 1.0256 - val_accuracy: 0.6440\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9450 - accuracy: 0.6958 - val_loss: 1.0296 - val_accuracy: 0.6435\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9430 - accuracy: 0.6974 - val_loss: 1.0311 - val_accuracy: 0.6430\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9413 - accuracy: 0.6987 - val_loss: 1.0335 - val_accuracy: 0.6437\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 87s 66ms/step - loss: 0.9412 - accuracy: 0.6996 - val_loss: 1.0355 - val_accuracy: 0.6441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2578c1b2a00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translating new sentences with our RNN encoder and decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I have to run.\n",
      "[start] tengo que [UNK] [end]\n",
      "-\n",
      "There were few people in the park.\n",
      "[start] había algunas personas en el parque [end]\n",
      "-\n",
      "I need to learn French.\n",
      "[start] necesito aprender francés [end]\n",
      "-\n",
      "It's been a long time since we last saw each other.\n",
      "[start] ha pasado mucho tiempo desde el tiempo [end]\n",
      "-\n",
      "You must know yourself.\n",
      "[start] debes saber a ti mismo [end]\n",
      "-\n",
      "Tom doesn't even know how to start a lawn mower.\n",
      "[start] tom ni siquiera sabe cómo [UNK] una clase de [UNK] [end]\n",
      "-\n",
      "Are they satisfied?\n",
      "[start] están ellos [end]\n",
      "-\n",
      "I should've paid a little more attention.\n",
      "[start] debería haber un poco más de la comida [end]\n",
      "-\n",
      "Tom hung out with his friends last night.\n",
      "[start] tom anoche con sus amigos [end]\n",
      "-\n",
      "Tom wanted to buy a new pair of tennis shoes.\n",
      "[start] tom quería comprar un par de zapatos por la noche de comprar [end]\n",
      "-\n",
      "We'll start when he gets here.\n",
      "[start] cuando él te [UNK] aquí [end]\n",
      "-\n",
      "Tom and I have fun together.\n",
      "[start] tom y yo nos juntos juntos [end]\n",
      "-\n",
      "Take your umbrella with you.\n",
      "[start] [UNK] tu paraguas [end]\n",
      "-\n",
      "I want to see you in my office this afternoon.\n",
      "[start] quiero ver a mi oficina en esta tarde [end]\n",
      "-\n",
      "Don't lecture me.\n",
      "[start] no me [UNK] [end]\n",
      "-\n",
      "He should arrive at the airport by 9 a.m.\n",
      "[start] debería ir al aeropuerto a las ocho [end]\n",
      "-\n",
      "She took care of the children.\n",
      "[start] ella se niños con los niños [end]\n",
      "-\n",
      "I just have a lot on my mind.\n",
      "[start] me tengo mucho en [UNK] [end]\n",
      "-\n",
      "They often say that life is short.\n",
      "[start] ellos a menudo que la vida es la [UNK] [end]\n",
      "-\n",
      "Tom is stupider than you think.\n",
      "[start] tom es más de lo que te parece que yo [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy 모델을 개선하는 방법:\n",
    "\n",
    "* 순환 레이어의 딥 스택(엔코더 및 디코더 모두)\n",
    "* GRU 대신 LSTM 사용\n",
    "* ...\n",
    "\n",
    "\n",
    "제한 사항\n",
    "\n",
    "* 소스 seq 표현은 인코더 상태(state) 벡터(크기, 소스 문장의 복잡성)에 유지되어야 합니다.\n",
    "* RNN은 점진적으로 과거를 잊어버립니다. 긴 시퀀스에 적합하지 않음\n",
    "\n",
    "Solution: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence learning with Transformer\n",
    "\n",
    "Neural attention: RNN이 처리할 수 있는 것보다 훨씬 더 길고 복잡한 시퀀스\n",
    "\n",
    "**Transformer Incoder** 는 소스 시퀀스를 읽고 인코딩된 표현을 생성합니다.\n",
    "\n",
    "RNN 인코더와 달리 Transformer 인코더는 인코딩된 표현을 시퀀스 형식으로 유지합니다. 이는 컨텍스트 인식(context-aware) 임베딩 벡터의 시퀀스입니다.\n",
    "\n",
    "**Transformer Decoder**: RNN 디코더와 마찬가지로 대상 시퀀스에서 토큰 0...N을 읽고 토큰 N+1을 예측하기 위해 연결합니다.\n",
    "\n",
    "하지만: Neural attention를 사용하여 인코딩된 소스 문장에서 현재 예측하려는 대상 토큰과 가장 밀접하게 관련된 토큰을 식별합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-14.png\" width=\"500\"><p style=\"text-align:center\">Figure 11.14 The TransformerDecoder is similar to the TransformerEncoder, except it features an additional attention block where the keys and values are the source sequence encoded by the TransformerEncoder. Together, the encoder and the decoder form an end-to-end Transformer.</p>\n",
    "\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/transformer_architecture.jpg\" width=\"500\">\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Transformer decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `TransformerDecoder`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sshleifer.github.io/blog_v2/images/copied_from_nb/seq2seq_dec.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together: A Transformer for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PositionalEmbedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End-to-end Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the sequence-to-sequence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 76s 57ms/step - loss: 1.7063 - accuracy: 0.4056 - val_loss: 1.3620 - val_accuracy: 0.4967\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 75s 57ms/step - loss: 1.3737 - accuracy: 0.5179 - val_loss: 1.2153 - val_accuracy: 0.5483\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 77s 59ms/step - loss: 1.2161 - accuracy: 0.5707 - val_loss: 1.1035 - val_accuracy: 0.5915\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 1.1189 - accuracy: 0.6053 - val_loss: 1.0640 - val_accuracy: 0.6135\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 1.0636 - accuracy: 0.6299 - val_loss: 1.0240 - val_accuracy: 0.6306\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 79s 60ms/step - loss: 1.0287 - accuracy: 0.6474 - val_loss: 1.0114 - val_accuracy: 0.6380\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.0034 - accuracy: 0.6618 - val_loss: 1.0000 - val_accuracy: 0.6475\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 0.9834 - accuracy: 0.6728 - val_loss: 1.0023 - val_accuracy: 0.6494\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9651 - accuracy: 0.6827 - val_loss: 1.0005 - val_accuracy: 0.6494\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9492 - accuracy: 0.6910 - val_loss: 0.9963 - val_accuracy: 0.6549\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9357 - accuracy: 0.6978 - val_loss: 0.9961 - val_accuracy: 0.6573\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9218 - accuracy: 0.7045 - val_loss: 0.9991 - val_accuracy: 0.6583\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9097 - accuracy: 0.7097 - val_loss: 0.9985 - val_accuracy: 0.6589\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8976 - accuracy: 0.7153 - val_loss: 1.0033 - val_accuracy: 0.6589\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8862 - accuracy: 0.7205 - val_loss: 1.0004 - val_accuracy: 0.6634\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8747 - accuracy: 0.7252 - val_loss: 1.0108 - val_accuracy: 0.6574\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8643 - accuracy: 0.7295 - val_loss: 1.0052 - val_accuracy: 0.6646\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8537 - accuracy: 0.7337 - val_loss: 1.0132 - val_accuracy: 0.6607\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8442 - accuracy: 0.7378 - val_loss: 1.0159 - val_accuracy: 0.6633\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8346 - accuracy: 0.7412 - val_loss: 1.0209 - val_accuracy: 0.6651\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8250 - accuracy: 0.7448 - val_loss: 1.0206 - val_accuracy: 0.6647\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8164 - accuracy: 0.7478 - val_loss: 1.0303 - val_accuracy: 0.6649\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8078 - accuracy: 0.7509 - val_loss: 1.0283 - val_accuracy: 0.6652\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7995 - accuracy: 0.7546 - val_loss: 1.0403 - val_accuracy: 0.6611\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7914 - accuracy: 0.7575 - val_loss: 1.0402 - val_accuracy: 0.6626\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7839 - accuracy: 0.7596 - val_loss: 1.0403 - val_accuracy: 0.6631\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7765 - accuracy: 0.7625 - val_loss: 1.0402 - val_accuracy: 0.6664\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7700 - accuracy: 0.7647 - val_loss: 1.0513 - val_accuracy: 0.6641\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7628 - accuracy: 0.7673 - val_loss: 1.0520 - val_accuracy: 0.6647\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7562 - accuracy: 0.7696 - val_loss: 1.0643 - val_accuracy: 0.6661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257a201d910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translating new sentences with our Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Who wants a drink?\n",
      "[start] quién quiere una paz [end]\n",
      "-\n",
      "\"What's happening in the cave? I'm curious.\" \"I have no idea.\"\n",
      "[start] qué ha salido de la [UNK] no tengo ni idea de salir [end]\n",
      "-\n",
      "He didn't attend the meeting.\n",
      "[start] no [UNK] [end]\n",
      "-\n",
      "I believe that the boy is honest.\n",
      "[start] creo que el niño es honesto [end]\n",
      "-\n",
      "It looks like this car is his.\n",
      "[start] parece que este coche es mary [end]\n",
      "-\n",
      "Tom asked Mary to warm up some leftovers.\n",
      "[start] tom le pidió a mary que se [UNK] algunos de los ojos [end]\n",
      "-\n",
      "Tom managed to tell me the truth.\n",
      "[start] tom me pidió que fuera la verdad [end]\n",
      "-\n",
      "He's depressed.\n",
      "[start] está [UNK] [end]\n",
      "-\n",
      "I ran home.\n",
      "[start] me encontré a la casa [end]\n",
      "-\n",
      "The result was really satisfying.\n",
      "[start] el resultado fue realmente [UNK] [end]\n",
      "-\n",
      "She failed to appear.\n",
      "[start] ella no Él [UNK] [end]\n",
      "-\n",
      "You can't park here. However, there is a parking lot just around the corner.\n",
      "[start] no puedes miedo por nadie [UNK] en un [UNK] por [UNK] sobre la isla [end]\n",
      "-\n",
      "Don't forget to close the door.\n",
      "[start] no te está olvidado cerrar la puerta [end]\n",
      "-\n",
      "A priest skillfully drew a picture of a priest on a folding screen.\n",
      "[start] un [UNK] [UNK] una foto de un [UNK] en el [end]\n",
      "-\n",
      "Unfortunately, my father isn't at home.\n",
      "[start] se se se se se se se se se se se se para usar en mi padre [end]\n",
      "-\n",
      "I want to go fishing.\n",
      "[start] quiero ir a pescar [end]\n",
      "-\n",
      "You certainly play the piano well.\n",
      "[start] tú [UNK] bien el piano [end]\n",
      "-\n",
      "Tom didn't get angry with Mary.\n",
      "[start] tom no se [UNK] con mary [end]\n",
      "-\n",
      "Tom shrugged.\n",
      "[start] tom se [UNK] [end]\n",
      "-\n",
      "I would rather go to the art museum than to the movie theater.\n",
      "[start] yo querría ir al verano pasado [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* 두 종류의 NLP 모델\n",
    "   - back-of-words 모델: 단어 세트 또는 N-grams (Dense Layers)\n",
    "   - 시퀀스 모델: 단어 순서(RNN, 1D convnet 또는 Transformer)\n",
    "   \n",
    "   \n",
    "* 텍스트 분류: `#samples / mean # words er sample` 을 사용하면 back-of-words 모델을 사용해야 하는지, 시퀀스 모델을 사용해야 하는지 결정하는 데 도움이 됩니다.\n",
    "\n",
    "\n",
    "* Sequence-to-sequence 학습은 기계 번역을 포함한 많은 NLP 문제를 해결하는 데 적용할 수 있는 일반적이고 강력한 학습 프레임워크입니다.\n",
    "   - 인코더: 소스 시퀀스를 처리\n",
    "   - 디코더: 과거 토큰과 인코더 처리된 소스 시퀀스를 보고 타겟 시퀀스에서 미래 토큰을 예측합니다.\n",
    "   \n",
    "   \n",
    "* Neural attention: 문맥 인식 단어 표현을 만드는 방법. Transformer 아키텍처의 기초입니다.\n",
    "\n",
    "\n",
    "* Transformer 아키텍처(TransformerEncoder, TransformerDecoder)는 sequence-to-sequence 작업에서 우수한 결과를 제공합니다. TransformerEncoder는 텍스트 분류 또는 모든 종류의 단일 입력 NLP 작업에도 사용할 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part03_transformer.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
