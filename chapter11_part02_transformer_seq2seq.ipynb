{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Ch.11.(Part 2) Deep learning for text\n",
    "\n",
    "Chapter contents:\n",
    "\n",
    "* Transformer 아키텍처\n",
    "* Sequence-to-sequence 학습\n",
    "\n",
    "\n",
    "## Ch.11.3 The Transformer architecture\n",
    "\n",
    "2017년부터 새로운 모델 아키텍처: Transformer(Attention is all you need)\n",
    "\n",
    "\"neural attention\"이라는 간단한 메커니즘을 사용하여 순환(recurrent) 레이어나 컨볼루션 레이어가 없는 강력한 시퀀스 모델을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Understanding self-attention\n",
    "\n",
    "모델이 보는 모든 입력 정보가 당면한 작업에 똑같이 중요하지는 않습니다.\n",
    "\n",
    "모델은 일부 특징에 \"더 많은 주의를 기울이고(pay more attention)\" 다른 특징에 \"덜 주의를 기울여야(pay less attention)\" 합니다.\n",
    "\n",
    "비슷한 생각:\n",
    "\n",
    "*  convnet의 Max pooling은 공간 영역의 특징 풀을 살펴보고 유지할 특징을 하나만 선택합니다.\n",
    "* TF-IDF 정규화는 다른 토큰이 전달할 가능성이 있는 정보의 양에 따라 토큰에 중요도 점수를 할당합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-05.png\" width=\"300\"><p style=\"text-align:center\">Figure 11.5 The general concept of “attention” in deep learning: input features get assigned “attention scores,” which can be used to inform the next representation of the input. From raw text to vectors.</p>\n",
    "\n",
    "Attention 메커니즘을 사용하여 특징을 컨텍스트-인식(context-aware)이 가능하게 할 수 있습니다.\n",
    "\n",
    "벡터 공간은 서로 다른 단어 간의 의미 관계의 \"모양\"을 캡처합니다.\n",
    "\n",
    "한 단어는 고정된 위치를 갖는다\n",
    "\n",
    "그러나: 단어의 의미는 일반적으로 문맥에 따라 다릅니다.\n",
    "\n",
    "스마트 임베딩 공간: 단어를 둘러싼 다른 단어에 따라 단어에 대해 다른 벡터 표현 제공\n",
    "\n",
    "self-attention의 목적은 시퀀스에서 관련 토큰의 표현을 사용하여 토큰의 표현을 변조하는 것입니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06.png\" width=\"500\"><p style=\"text-align:center\">Figure 11.6. Self-attention: attention scores are computed between “station” and every other word in the sequence, and they are then used to weight a sum of word vectors that becomes the new “station” vector.</p>\n",
    "\n",
    "* Compute relevancy scores (dot product)\n",
    "* Scale (1/sqrt) and softmax\n",
    "* Weighted (scores) sum of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=input_sequence.shape)\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        scores = np.zeros(shape=(len(input_sequence),))\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector, vector.T)\n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = softmax(scores)\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += vector * scores[j]\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Generalized self-attention: the query-key-value model\n",
    "\n",
    "self-attention을 위한 백터화 구현\n",
    "```\n",
    "    num_heads = 4 \n",
    "    embed_dim = 256 \n",
    "    mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    outputs = mha_layer(inputs, inputs, inputs)\n",
    "```\n",
    "지금까지 단 하나의 시퀀스를 사용함. \n",
    "그러나 기계 번역용으로 개발된 Transformer (2개의 시퀀스: source, target)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06-UN01.png\" width=\"450\">\n",
    "\n",
    "* 입력(A)의 각 토큰에 대해, \n",
    "* 토큰이 입력(B)의 모든 토큰과 얼마나 관련되어 있는지 계산\n",
    "* 이 점수를 사용하여 입력(C)의 토큰 합계에 가중치를 부여합니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-06-UN02.png\" width=\"450\">\n",
    " \n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-07.png\" width=\"500\"><p style=\"text-align:center\">Figure 11.7 Retrieving images from a database: the “query” is compared to a set of “keys,” and the match scores are used to rank “values” (images).</p>\n",
    "\n",
    "* Source: \"How's the weather today?\"\n",
    "* Target: \"¿Qué tiempo hace hoy?\" \n",
    "\n",
    "* query: target\n",
    "* keys: source\n",
    "* values: source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "다중 헤드(Multi-head): self-attention layer의 출력 공간은 개별적으로 학습된 독립적인 부분 공간 세트로 나누어집니다.\n",
    "\n",
    "초기 쿼리(query), 키(key) 및 값(value)은 3개의 독립적인 dense projections 세트를 통해 전송되어 3개의 개별 벡터가 생성됩니다.\n",
    "\n",
    "각 벡터는 neural attention 을 통해 처리됩니다.\n",
    "\n",
    "3개의 출력은 다시 하나의 출력 시퀀스로 연결됩니다.\n",
    "\n",
    "학습 가능한 dense projections의 존재는 레이어가 무언가를 학습할 수 있도록 합니다(순수한 상태없는 변환(stateless transformation)과 반대).\n",
    "\n",
    "독립 헤드(Independent heads): 레이어가 각 토큰에 대한 다양한 특징 그룹을 학습하도록 돕습니다.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-08.png\" width=\"400\"><p style=\"text-align:center\">Figure 11.8 The MultiHeadAttention layer</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Transformer encoder\n",
    "\n",
    "Dense projections 작업 -> attention 메커니즘의 출력에도 추가함.\n",
    "\n",
    "또한\n",
    "\n",
    "* residual connection 추가(깊어지기 때문에)\n",
    "* 정규화 추가(역전파 동안 더 나은 gradients 흐름을 위해)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-09.png\" width=\"200\"><p style=\"text-align:center\">Figure 11.9 The TransformerEncoder chains a MultiHeadAttention layer with a dense projection and adds normalization as well as residual connections.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# import os, pathlib, shutil, random\n",
    "# from tensorflow import keras\n",
    "# batch_size = 32\n",
    "# base_dir = pathlib.Path(\"aclImdb\")\n",
    "# val_dir = base_dir / \"val\"\n",
    "# train_dir = base_dir / \"train\"\n",
    "# for category in (\"neg\", \"pos\"):\n",
    "#     os.makedirs(val_dir / category)\n",
    "#     files = os.listdir(train_dir / category)\n",
    "#     random.Random(1337).shuffle(files)\n",
    "#     num_val_samples = int(0.2 * len(files))\n",
    "#     val_files = files[-num_val_samples:]\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(train_dir / category / fname,\n",
    "#                     val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 600), dtype=int64, numpy=\n",
       " array([[    4,   473,     5, ...,     0,     0,     0],\n",
       "        [ 9793,     7,   445, ...,     0,     0,     0],\n",
       "        [  615,    10,  2449, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [14541,     9,     7, ...,     0,     0,     0],\n",
       "        [   10,   118,    12, ...,     0,     0,     0],\n",
       "        [ 9493,     2,  1290, ...,     0,     0,     0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1])>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(int_test_ds.take(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Transformer encoder implemented as a subclassed `Layer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the Transformer encoder for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 256)        543776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and evaluating the Transformer encoder based model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 37s 51ms/step - loss: 0.4791 - accuracy: 0.7758 - val_loss: 0.3446 - val_accuracy: 0.8504\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.3156 - accuracy: 0.8662 - val_loss: 0.3205 - val_accuracy: 0.8640\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.2486 - accuracy: 0.8988 - val_loss: 0.2945 - val_accuracy: 0.8852\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.1933 - accuracy: 0.9245 - val_loss: 0.3295 - val_accuracy: 0.8874\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.1534 - accuracy: 0.9421 - val_loss: 0.3528 - val_accuracy: 0.8836\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1268 - accuracy: 0.9533 - val_loss: 0.3746 - val_accuracy: 0.8910\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.1124 - accuracy: 0.9577 - val_loss: 0.3498 - val_accuracy: 0.8884\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.0973 - accuracy: 0.9642 - val_loss: 0.3889 - val_accuracy: 0.8840\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.0847 - accuracy: 0.9703 - val_loss: 0.4278 - val_accuracy: 0.8844\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0722 - accuracy: 0.9738 - val_loss: 0.4322 - val_accuracy: 0.8764\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0665 - accuracy: 0.9758 - val_loss: 0.4791 - val_accuracy: 0.8800\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0570 - accuracy: 0.9800 - val_loss: 0.5492 - val_accuracy: 0.8764\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.5954 - val_accuracy: 0.8662\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0450 - accuracy: 0.9848 - val_loss: 0.6454 - val_accuracy: 0.8678\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 32s 52ms/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 0.6297 - val_accuracy: 0.8708\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0305 - accuracy: 0.9894 - val_loss: 0.9021 - val_accuracy: 0.8536\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0251 - accuracy: 0.9924 - val_loss: 0.7875 - val_accuracy: 0.8632\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.7832 - val_accuracy: 0.8692\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 33s 52ms/step - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.7951 - val_accuracy: 0.8598\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.0166 - accuracy: 0.9959 - val_loss: 0.9597 - val_accuracy: 0.8604\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.3078 - accuracy: 0.8737\n",
      "Test acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872\n",
    "* glove_embeddings_sequence model: 0.868\n",
    "* transformer_encoder: 0.874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 모델보다 좋지 않습니다. 왜요?\n",
    "\n",
    "현재 Transformer 모델: 시퀀스 모델이 아닙니다.\n",
    "\n",
    "Attention 계층: 토큰을 집합(set)으로 봅니다.\n",
    "\n",
    "토큰 순서 변경: 동일한 컨텍스트 인식 표현\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-10.png\" width=\"250\"><p style=\"text-align:center\">Figure 11.10 Features of different types of NLP models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Using positional encoding to re-inject order information\n",
    "\n",
    "\n",
    "위치 인코딩: 모델에 단어-순서 정보에 대한 액세스 부여\n",
    "\n",
    "각 단어 임베딩에 단어의 위치 추가\n",
    "\n",
    "워드 임베딩 구성요소:\n",
    "* 단어 벡터(word vector)(독립 표현)\n",
    "* 위치 벡터(position vector)(단어의 위치를 나타냄)\n",
    "\n",
    "구현\n",
    "\n",
    "모델은 위치-임베딩(position-embedding) 벡터를 학습합니다(단어 인덱스를 포함하는 것과 동일한 방식)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Implementing positional embedding as a subclassed layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Putting it all together: A text-classification Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Combining the Transformer encoder with positional embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        5273600   \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 256)        543776    \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 34s 52ms/step - loss: 0.4635 - accuracy: 0.7948 - val_loss: 0.3083 - val_accuracy: 0.8752\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 33s 53ms/step - loss: 0.2334 - accuracy: 0.9084 - val_loss: 0.2732 - val_accuracy: 0.8910\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 34s 55ms/step - loss: 0.1730 - accuracy: 0.9347 - val_loss: 0.3278 - val_accuracy: 0.8872\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1429 - accuracy: 0.9478 - val_loss: 0.3534 - val_accuracy: 0.8876\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.1197 - accuracy: 0.9564 - val_loss: 0.4965 - val_accuracy: 0.8752\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.1071 - accuracy: 0.9632 - val_loss: 0.3559 - val_accuracy: 0.8814\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.0913 - accuracy: 0.9689 - val_loss: 0.4292 - val_accuracy: 0.8802\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0833 - accuracy: 0.9715 - val_loss: 0.4113 - val_accuracy: 0.8776\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0737 - accuracy: 0.9754 - val_loss: 0.4758 - val_accuracy: 0.8764\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0637 - accuracy: 0.9785 - val_loss: 0.5045 - val_accuracy: 0.8656\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0551 - accuracy: 0.9818 - val_loss: 0.5299 - val_accuracy: 0.8728\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 35s 56ms/step - loss: 0.0506 - accuracy: 0.9840 - val_loss: 0.5183 - val_accuracy: 0.8730\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0442 - accuracy: 0.9858 - val_loss: 0.5358 - val_accuracy: 0.8672\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0396 - accuracy: 0.9863 - val_loss: 0.4978 - val_accuracy: 0.8708\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0341 - accuracy: 0.9893 - val_loss: 0.7568 - val_accuracy: 0.8670\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 35s 57ms/step - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.7746 - val_accuracy: 0.8678\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0258 - accuracy: 0.9914 - val_loss: 0.7611 - val_accuracy: 0.8708\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0258 - accuracy: 0.9927 - val_loss: 0.9212 - val_accuracy: 0.8668\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0186 - accuracy: 0.9937 - val_loss: 1.0651 - val_accuracy: 0.8732\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.9159 - val_accuracy: 0.8670\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.2981 - accuracy: 0.8820\n",
      "Test acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872\n",
    "* glove_embeddings_sequence model: 0.868\n",
    "* transformer_encoder: 0.874\n",
    "* full_transformer_encoder: 0.882\n",
    "\n",
    "Improved but still not as good as the Bag-of-words ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### When to use sequence models over bag-of-words models?\n",
    "\n",
    "Bag-of-words는 구식이 아닙니다.\n",
    "\n",
    "어떤 경우에는 bag-of-bigrams 위에 Dense layers의 작은 스택이 완벽하게 유효합니다.\n",
    "\n",
    "다양한 유형의 텍스트 데이터 세트에 대한 다양한 텍스트 분류 기술 분석\n",
    "\n",
    "중요 기준: 훈련 데이터의 샘플 수와 샘플당 평균 단어 수 간의 비율\n",
    "\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-11.png\" width=\"350\"><p style=\"text-align:center\">Figure 11.11 A simple heuristic for selecting a text-classification model: the ratio between the number of training samples and the mean number of words per sample</p>\n",
    "\n",
    "Sequence models work best when\n",
    "\n",
    "* lots of training data is available\n",
    "* each sample is relatively short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond text classification: Sequence-to-sequence learning\n",
    "\n",
    "Sequence-to-sequence learning applications\n",
    "\n",
    "* Machine translation\n",
    "* Text summarization\n",
    "* Chatbots\n",
    "* Text generation\n",
    "\n",
    "\n",
    "General template:\n",
    " * Encoder: turns the source sequence into an intermediate representation\n",
    " * Decoder: trained to predict the next token i in the target sequence by looking at\n",
    "     * previous tokens (0 to i-1) and\n",
    "     * the encoded source sequence\n",
    "     \n",
    "During Inference: no access to target sequence. Generate it one token at a time\n",
    "\n",
    "1. obtain the encoded source sequence from the encoder\n",
    "2. decoder starts with:\n",
    "  * encoded source sequence\n",
    "  * initial \"seed\" token [start]\n",
    "  \n",
    "  predicts the first real token in the sequence\n",
    "  \n",
    "3. Predicted sequences fed back into the decoder (until stop token [end] is generated)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-12.png\" width=\"450\"><p style=\"text-align:center\">Figure 11.12 Sequence-to-sequence learning: the source sequence is processed by the encoder and is then sent to the decoder. The decoder looks at the target sequence so far and predicts the target sequence offset by one step in the future. During inference, we generate one target token at a time and feed it back into the decoder.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A machine translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tom waited patiently for Mary.', '[start] Tom esperó pacientemente a Mary. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing the English and Spanish text pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing datasets for the translation task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence learning with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU-based encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU-based decoder and the end-to-end model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training our recurrent sequence-to-sequence model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 91s 64ms/step - loss: 1.6376 - accuracy: 0.4166 - val_loss: 1.3168 - val_accuracy: 0.5069\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 1.3157 - accuracy: 0.5255 - val_loss: 1.1546 - val_accuracy: 0.5691\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 1.1735 - accuracy: 0.5757 - val_loss: 1.0726 - val_accuracy: 0.6003\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0828 - accuracy: 0.6076 - val_loss: 1.0386 - val_accuracy: 0.6204\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0358 - accuracy: 0.6326 - val_loss: 1.0209 - val_accuracy: 0.6298\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0050 - accuracy: 0.6507 - val_loss: 1.0166 - val_accuracy: 0.6363\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 85s 65ms/step - loss: 0.9859 - accuracy: 0.6638 - val_loss: 1.0163 - val_accuracy: 0.6391\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 85s 66ms/step - loss: 0.9724 - accuracy: 0.6744 - val_loss: 1.0191 - val_accuracy: 0.6411\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 87s 66ms/step - loss: 0.9622 - accuracy: 0.6824 - val_loss: 1.0221 - val_accuracy: 0.6410\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9546 - accuracy: 0.6880 - val_loss: 1.0235 - val_accuracy: 0.6438\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9491 - accuracy: 0.6926 - val_loss: 1.0256 - val_accuracy: 0.6440\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9450 - accuracy: 0.6958 - val_loss: 1.0296 - val_accuracy: 0.6435\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9430 - accuracy: 0.6974 - val_loss: 1.0311 - val_accuracy: 0.6430\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 0.9413 - accuracy: 0.6987 - val_loss: 1.0335 - val_accuracy: 0.6437\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 87s 66ms/step - loss: 0.9412 - accuracy: 0.6996 - val_loss: 1.0355 - val_accuracy: 0.6441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2578c1b2a00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translating new sentences with our RNN encoder and decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I have to run.\n",
      "[start] tengo que [UNK] [end]\n",
      "-\n",
      "There were few people in the park.\n",
      "[start] había algunas personas en el parque [end]\n",
      "-\n",
      "I need to learn French.\n",
      "[start] necesito aprender francés [end]\n",
      "-\n",
      "It's been a long time since we last saw each other.\n",
      "[start] ha pasado mucho tiempo desde el tiempo [end]\n",
      "-\n",
      "You must know yourself.\n",
      "[start] debes saber a ti mismo [end]\n",
      "-\n",
      "Tom doesn't even know how to start a lawn mower.\n",
      "[start] tom ni siquiera sabe cómo [UNK] una clase de [UNK] [end]\n",
      "-\n",
      "Are they satisfied?\n",
      "[start] están ellos [end]\n",
      "-\n",
      "I should've paid a little more attention.\n",
      "[start] debería haber un poco más de la comida [end]\n",
      "-\n",
      "Tom hung out with his friends last night.\n",
      "[start] tom anoche con sus amigos [end]\n",
      "-\n",
      "Tom wanted to buy a new pair of tennis shoes.\n",
      "[start] tom quería comprar un par de zapatos por la noche de comprar [end]\n",
      "-\n",
      "We'll start when he gets here.\n",
      "[start] cuando él te [UNK] aquí [end]\n",
      "-\n",
      "Tom and I have fun together.\n",
      "[start] tom y yo nos juntos juntos [end]\n",
      "-\n",
      "Take your umbrella with you.\n",
      "[start] [UNK] tu paraguas [end]\n",
      "-\n",
      "I want to see you in my office this afternoon.\n",
      "[start] quiero ver a mi oficina en esta tarde [end]\n",
      "-\n",
      "Don't lecture me.\n",
      "[start] no me [UNK] [end]\n",
      "-\n",
      "He should arrive at the airport by 9 a.m.\n",
      "[start] debería ir al aeropuerto a las ocho [end]\n",
      "-\n",
      "She took care of the children.\n",
      "[start] ella se niños con los niños [end]\n",
      "-\n",
      "I just have a lot on my mind.\n",
      "[start] me tengo mucho en [UNK] [end]\n",
      "-\n",
      "They often say that life is short.\n",
      "[start] ellos a menudo que la vida es la [UNK] [end]\n",
      "-\n",
      "Tom is stupider than you think.\n",
      "[start] tom es más de lo que te parece que yo [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-sequence learning with Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Transformer decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `TransformerDecoder`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together: A Transformer for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PositionalEmbedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End-to-end Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the sequence-to-sequence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 76s 57ms/step - loss: 1.7063 - accuracy: 0.4056 - val_loss: 1.3620 - val_accuracy: 0.4967\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 75s 57ms/step - loss: 1.3737 - accuracy: 0.5179 - val_loss: 1.2153 - val_accuracy: 0.5483\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 77s 59ms/step - loss: 1.2161 - accuracy: 0.5707 - val_loss: 1.1035 - val_accuracy: 0.5915\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 1.1189 - accuracy: 0.6053 - val_loss: 1.0640 - val_accuracy: 0.6135\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 1.0636 - accuracy: 0.6299 - val_loss: 1.0240 - val_accuracy: 0.6306\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 79s 60ms/step - loss: 1.0287 - accuracy: 0.6474 - val_loss: 1.0114 - val_accuracy: 0.6380\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 1.0034 - accuracy: 0.6618 - val_loss: 1.0000 - val_accuracy: 0.6475\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 79s 61ms/step - loss: 0.9834 - accuracy: 0.6728 - val_loss: 1.0023 - val_accuracy: 0.6494\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9651 - accuracy: 0.6827 - val_loss: 1.0005 - val_accuracy: 0.6494\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9492 - accuracy: 0.6910 - val_loss: 0.9963 - val_accuracy: 0.6549\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 80s 61ms/step - loss: 0.9357 - accuracy: 0.6978 - val_loss: 0.9961 - val_accuracy: 0.6573\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9218 - accuracy: 0.7045 - val_loss: 0.9991 - val_accuracy: 0.6583\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9097 - accuracy: 0.7097 - val_loss: 0.9985 - val_accuracy: 0.6589\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8976 - accuracy: 0.7153 - val_loss: 1.0033 - val_accuracy: 0.6589\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.8862 - accuracy: 0.7205 - val_loss: 1.0004 - val_accuracy: 0.6634\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8747 - accuracy: 0.7252 - val_loss: 1.0108 - val_accuracy: 0.6574\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8643 - accuracy: 0.7295 - val_loss: 1.0052 - val_accuracy: 0.6646\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8537 - accuracy: 0.7337 - val_loss: 1.0132 - val_accuracy: 0.6607\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8442 - accuracy: 0.7378 - val_loss: 1.0159 - val_accuracy: 0.6633\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8346 - accuracy: 0.7412 - val_loss: 1.0209 - val_accuracy: 0.6651\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8250 - accuracy: 0.7448 - val_loss: 1.0206 - val_accuracy: 0.6647\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8164 - accuracy: 0.7478 - val_loss: 1.0303 - val_accuracy: 0.6649\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.8078 - accuracy: 0.7509 - val_loss: 1.0283 - val_accuracy: 0.6652\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7995 - accuracy: 0.7546 - val_loss: 1.0403 - val_accuracy: 0.6611\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7914 - accuracy: 0.7575 - val_loss: 1.0402 - val_accuracy: 0.6626\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7839 - accuracy: 0.7596 - val_loss: 1.0403 - val_accuracy: 0.6631\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7765 - accuracy: 0.7625 - val_loss: 1.0402 - val_accuracy: 0.6664\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7700 - accuracy: 0.7647 - val_loss: 1.0513 - val_accuracy: 0.6641\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.7628 - accuracy: 0.7673 - val_loss: 1.0520 - val_accuracy: 0.6647\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 81s 62ms/step - loss: 0.7562 - accuracy: 0.7696 - val_loss: 1.0643 - val_accuracy: 0.6661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x257a201d910>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Translating new sentences with our Transformer model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Who wants a drink?\n",
      "[start] quién quiere una paz [end]\n",
      "-\n",
      "\"What's happening in the cave? I'm curious.\" \"I have no idea.\"\n",
      "[start] qué ha salido de la [UNK] no tengo ni idea de salir [end]\n",
      "-\n",
      "He didn't attend the meeting.\n",
      "[start] no [UNK] [end]\n",
      "-\n",
      "I believe that the boy is honest.\n",
      "[start] creo que el niño es honesto [end]\n",
      "-\n",
      "It looks like this car is his.\n",
      "[start] parece que este coche es mary [end]\n",
      "-\n",
      "Tom asked Mary to warm up some leftovers.\n",
      "[start] tom le pidió a mary que se [UNK] algunos de los ojos [end]\n",
      "-\n",
      "Tom managed to tell me the truth.\n",
      "[start] tom me pidió que fuera la verdad [end]\n",
      "-\n",
      "He's depressed.\n",
      "[start] está [UNK] [end]\n",
      "-\n",
      "I ran home.\n",
      "[start] me encontré a la casa [end]\n",
      "-\n",
      "The result was really satisfying.\n",
      "[start] el resultado fue realmente [UNK] [end]\n",
      "-\n",
      "She failed to appear.\n",
      "[start] ella no Él [UNK] [end]\n",
      "-\n",
      "You can't park here. However, there is a parking lot just around the corner.\n",
      "[start] no puedes miedo por nadie [UNK] en un [UNK] por [UNK] sobre la isla [end]\n",
      "-\n",
      "Don't forget to close the door.\n",
      "[start] no te está olvidado cerrar la puerta [end]\n",
      "-\n",
      "A priest skillfully drew a picture of a priest on a folding screen.\n",
      "[start] un [UNK] [UNK] una foto de un [UNK] en el [end]\n",
      "-\n",
      "Unfortunately, my father isn't at home.\n",
      "[start] se se se se se se se se se se se se para usar en mi padre [end]\n",
      "-\n",
      "I want to go fishing.\n",
      "[start] quiero ir a pescar [end]\n",
      "-\n",
      "You certainly play the piano well.\n",
      "[start] tú [UNK] bien el piano [end]\n",
      "-\n",
      "Tom didn't get angry with Mary.\n",
      "[start] tom no se [UNK] con mary [end]\n",
      "-\n",
      "Tom shrugged.\n",
      "[start] tom se [UNK] [end]\n",
      "-\n",
      "I would rather go to the art museum than to the movie theater.\n",
      "[start] yo querría ir al verano pasado [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part03_transformer.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
