{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Deep learning for text\n",
    "\n",
    "Chapter contents:\n",
    "* 머신 러닝 응용을 위한 텍스트 데이터 전처리\n",
    "* 텍스트 처리를 위한 Bag-of-words 접근법 및 시퀀스 모델링 접근법\n",
    "* Transformer 아키텍처\n",
    "* Sequence-to-sequence 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Natural-language processing: The bird's eye view\n",
    "\n",
    "NLP 작업들:\n",
    "\n",
    "* \"이 텍스트의 주제는 무엇입니까?\" (**텍스트 분류**)\n",
    "* \"이 텍스트에 욕설이 포함되어 있습니까?\" (**콘텐츠 필터링**)\n",
    "* \"이 텍스트는 긍정적으로 들리나요, 아니면 부정적으로 들리나요?\" (**감정분석(sentiment analysis)**)\n",
    "* \"이 불완전한 문장에서 다음 단어는 무엇이어야 합니까?\" (**언어 모델링**)\n",
    "* \"이것은 독일어로 어떻게 말할까요?\" (**번역**)\n",
    "* \"이 글을 한 단락으로 요약한다면?\" (**요약**)\n",
    "\n",
    "\n",
    "Short history:\n",
    "* 1990-2010: 의사결정나무(decision trees), 로지스틱 회귀 등(feature engineering)\n",
    "* 2015-2017: LSTM (Keras에서 최초로 사용하기 쉬운 오픈 소스 구현)\n",
    "* 2017-현재: 트랜스포머(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Preparing text data\n",
    "\n",
    "미분 가능한 함수들로 구성된 딥 러닝 모델은 숫자 열만 처리할 수 있습니다. 원시 텍스트를 입력으로 사용할 수 없습니다. 텍스트를 숫자 텐서로 변환해야합니다.\n",
    "\n",
    "Text $\\rightarrow$ numeric tensor: Vectorization\n",
    "\n",
    "**Vectorization**\n",
    "\n",
    "* 표준화(Standardization): 텍스트 표준화(소문자화, 구두점 제거 등)\n",
    "* 토큰화(Tokenization): 단위(토큰)로 분할(예: 문자, 단어, 단어 그룹)\n",
    "* 인덱싱(Indexing) : Tokens $\\rightarrow$ 숫자 벡터\n",
    "\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-01.png\" width=\"400\"><p style=\"text-align:center\">Figure 11.1 From raw text to vectors.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text standardization\n",
    "\n",
    "\n",
    "* “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”\n",
    "* “Sunset came; I stared at the México sky. Isn’t nature splendid?”\n",
    "\n",
    "becomes\n",
    "\n",
    "* “sunset came i was staring at the mexico sky isnt nature splendid”\n",
    "* “sunset came i stared at the méxico sky isnt nature splendid”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Text splitting (tokenization)\n",
    "\n",
    "* 단어 수준(Word-level) 토큰화(공백 또는 구두점으로 구분된 하위 문자열)\n",
    "* N-gram 토큰화: 토큰은 N개의 연속된 단어로 그룹화됩니다(예: \"the cat\" 또는 \"he was\"는 2-gram 토큰(bigram이라고도 함))\n",
    "* 문자 수준(Character-level) 토큰화: 각 문자는 고유한 토큰입니다. (드물게 사용되는)\n",
    "\n",
    "Text-processing models\n",
    "\n",
    "* Sequence models (word-level tokenization)\n",
    " \n",
    "* Bag-of-words models (N-gram tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Vocabulary indexing\n",
    "\n",
    "각 토큰을 숫자 표현으로 인코딩\n",
    "\n",
    "* 훈련 데이터(\"어휘(vocabulary)\")에서 발견된 모든 용어의 색인을 작성합니다.\n",
    "* 어휘(vocabulary)의 각 항목에 고유한 정수 할당\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Example (not for run)\n",
    "\n",
    "vocabulary = {} \n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "            \n",
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1 \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 사용하는 두 가지 특수 토큰(two special tokens)이 있습니다. OOV 토큰(인덱스 1)과 마스크 토큰(인덱스 0)입니다.\n",
    "\n",
    "* \"어휘 외(out of vocabulary)\" 인덱스(OOV 인덱스로 축약됨): 인덱스에 없는 모든 토큰에 대한 포괄.\n",
    "    - ```token_index = volabulary.get(token, 1)```\n",
    "\n",
    "\n",
    "* 마스크 토큰(mask token), 시퀀스 크기가 다른 배치를 만드는 데 사용(인덱스 0),\n",
    "   - [5,7,124,4,89], [8,34,21]\n",
    "   - [5,7,124,4,89], [8,34,21,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using the TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 좋지 않으므로 Keras의 TextVectorization을 사용하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 설정\n",
    "\n",
    "* 텍스트 표준화(stardardization)를 위한 \"소문자로 변환하고, 구두점 제거\"\n",
    "* 토큰화(tokenization)를 위한 \"공백으로 분할\"\n",
    "\n",
    "사용자 정의 함수(custom functions)도 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 말뭉치(corpus)의 어휘를 인덱싱하려면 ```adapt()``` 메서드를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Two approaches for representing groups of words: Sets and sequences\n",
    "\n",
    "* 간단한 접근법: 텍스트 = 정렬되지 않은 단어 세트(bag-of-words 모델)\n",
    "* 순서 문제: 텍스트 = 단어의 단계(시계열과 유사)(recurrent 모델)\n",
    "* 하이브리드: 순서에 구애받지 않지만, 단어 위치 정보 포함(Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy\n"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Displaying the shapes and dtypes of the first batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'This film deals with the atrocity in Derry 30 years ago which is commonly known as Bloody Sunday.<br /><br />The film is well researched, acted and directed. It is as close to the truth as we will get until the outcome of the Saville enquiry. The film puts the atrocity into context of the time. It also shows the savagery of the soldiers on the day of the atrocity. The disgraceful white-wash that was the Widgery Tribunal is also dealt with.<br /><br />Overall, this is an excellent drama which is moving and shocking. When the Saville report comes out, watch this film again to see how close to the truth it is.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Processing words as a set: The bag-of-words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Single words (unigrams) with binary encoding\n",
    "\n",
    "\"the cat sat on the mat\" $\\rightarrow$ {\"cat\",\"mat\",\"on\",\"sat\",\"the\"}\n",
    "\n",
    "텍스트를 멀티-핫 벡터(multi-hot vector)로 인코딩\n",
    "\n",
    "* 어휘에 있는 단어수만큼의 차원을 가진 벡터\n",
    "* 이진법: 0은 부재, 1은 텍스트에 단어가 있음을 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preprocessing our datasets with a `TextVectorization` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Inspecting the output of our binary unigram dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Our model-building utility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary unigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 10ms/step - loss: 0.3944 - accuracy: 0.8354 - val_loss: 0.2971 - val_accuracy: 0.8786\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2659 - accuracy: 0.9026 - val_loss: 0.3024 - val_accuracy: 0.8876\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2349 - accuracy: 0.9175 - val_loss: 0.3196 - val_accuracy: 0.8922\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2214 - accuracy: 0.9246 - val_loss: 0.3345 - val_accuracy: 0.8914\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2156 - accuracy: 0.9291 - val_loss: 0.3615 - val_accuracy: 0.8844\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2117 - accuracy: 0.9322 - val_loss: 0.3667 - val_accuracy: 0.8846\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2076 - accuracy: 0.9348 - val_loss: 0.3742 - val_accuracy: 0.8876\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2041 - accuracy: 0.9368 - val_loss: 0.3877 - val_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2025 - accuracy: 0.9377 - val_loss: 0.3923 - val_accuracy: 0.8836\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2071 - accuracy: 0.9344 - val_loss: 0.4012 - val_accuracy: 0.8826\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2889 - accuracy: 0.8871\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with binary encoding\n",
    "\n",
    "\"the cat sat on the mat\" $\\rightarrow$ {\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the binary bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.3707 - accuracy: 0.8489 - val_loss: 0.2908 - val_accuracy: 0.8866\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2422 - accuracy: 0.9160 - val_loss: 0.3087 - val_accuracy: 0.8906\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2092 - accuracy: 0.9317 - val_loss: 0.3194 - val_accuracy: 0.8930\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1871 - accuracy: 0.9402 - val_loss: 0.3290 - val_accuracy: 0.8938\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1823 - accuracy: 0.9452 - val_loss: 0.3449 - val_accuracy: 0.8934\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1801 - accuracy: 0.9487 - val_loss: 0.3590 - val_accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1728 - accuracy: 0.9503 - val_loss: 0.3635 - val_accuracy: 0.8924\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1756 - accuracy: 0.9530 - val_loss: 0.3972 - val_accuracy: 0.8894\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1657 - accuracy: 0.9548 - val_loss: 0.4207 - val_accuracy: 0.8860\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1775 - accuracy: 0.9532 - val_loss: 0.3820 - val_accuracy: 0.8928\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.2798 - accuracy: 0.8926\n",
      "Test acc: 0.893\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.889\n",
    "* binary_2gram: 0.896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Bigrams with TF-IDF encoding\n",
    "\n",
    "\"the cat sat on the mat\" $\\rightarrow$  {\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1, \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring the `TextVectorization` layer to return token counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**\n",
    "\n",
    "* term frequency: 문서에 주어진 용어(term)가 더 많이 나타날수록 해당 용어는 문서의 내용을 이해하는 데 더 중요합니다. \n",
    "\n",
    "* inverse document frequency : 동시에 데이터 세트의 모든 문서에서 용어가 나타나는 빈도도 중요합니다. 거의 모든 문서에 나타나는 용어(예: \"the\" 또는 \"a\")는 특히 유익한 정보가 아닙니다. 반면에 모든 텍스트의 작은 부분 집합에만 나타나는 용어는 매우 구별되며 중요합니다. \n",
    "\n",
    "* TF-IDF(term frequency-inverse document frequency)는 이 두 가지 아이디어를 융합한 메트릭입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# not for execution\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training and testing the TF-IDF bigram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.4668 - accuracy: 0.8137 - val_loss: 0.2834 - val_accuracy: 0.8944\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2862 - accuracy: 0.8896 - val_loss: 0.2884 - val_accuracy: 0.8912\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2411 - accuracy: 0.9089 - val_loss: 0.3335 - val_accuracy: 0.8952\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2266 - accuracy: 0.9140 - val_loss: 0.3127 - val_accuracy: 0.8950\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2159 - accuracy: 0.9143 - val_loss: 0.3339 - val_accuracy: 0.8944\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2133 - accuracy: 0.9178 - val_loss: 0.3341 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1965 - accuracy: 0.9191 - val_loss: 0.3518 - val_accuracy: 0.8836\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2032 - accuracy: 0.9175 - val_loss: 0.3535 - val_accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1976 - accuracy: 0.9196 - val_loss: 0.3490 - val_accuracy: 0.8820\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1956 - accuracy: 0.9208 - val_loss: 0.3939 - val_accuracy: 0.8626\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2789 - accuracy: 0.8941\n",
      "Test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):    # this if it does not run on GPU\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.889\n",
    "* binary_2gram: 0.896\n",
    "* tfidf_2gram: 0.892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting A Model That Processes Raw Strings\n",
    "\n",
    "전처리(standardization, splitting, and indexing)는 tf.data 파이프라인의 일부였습니다.\n",
    "\n",
    "독립형(standalone) 솔루션에는 적합하지 않으므로, 추론 환경(예: 다른 언어/OS/하드웨어 등)에서 전처리를 다시 구현해야 합니다.\n",
    "\n",
    "솔루션: 모델에 전처리 포함시킵니다.(원시 데이터 -> 모델 -> 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.21 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing words as a sequence: The sequence model approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A first practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing integer sequence datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 300     # original 600 is too big to handle on a notebook GPU\n",
    "max_tokens = 10000   # original 20000 is too big to handle on a notebook GPU\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 300)\n",
      "inputs.dtype: <dtype: 'int64'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(\n",
      "[  11   14    4  890   18    9  626   46  179   50    4 1124  480 1402\n",
      "  498 3699 1213    5  655    3 5903   15  255 7260  117   25 5078   13\n",
      "  360  252   25  186  786  131   10   96  357   10   14    8   16    4\n",
      "  145  929  934   18  328  113   10  369   86  264    7    9    6 1069\n",
      "    4 1128    3  507   39    4 1051  128  439   96   22  101    2  338\n",
      "  137    5  255 7260   72   75    2  611 1071    3    2   18    1   13\n",
      "   36   11  218   21    9    7   83  113  194 8761    2 2441 9221    5\n",
      "  536    3  860  227  338 4430   13   12  288   10  497    2   18    4\n",
      "  694   80    8 2708    5   30 1585    9  117 4339    4  423    5    1\n",
      "   12   10   42   96   22  178 4428 4710   13   10   81   22  362   11\n",
      "   18   19   10   26    6  920   10   26  294  441    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0], shape=(300,), dtype=int64)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in int_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A sequence model built on one-hot encoded vector sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 10000)       0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               2568448   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,568,513\n",
      "Trainable params: 2,568,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a first basic sequence model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 45s 68ms/step - loss: 0.5101 - accuracy: 0.7645 - val_loss: 0.3687 - val_accuracy: 0.8538\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 43s 69ms/step - loss: 0.3428 - accuracy: 0.8756 - val_loss: 0.3300 - val_accuracy: 0.8730\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 44s 70ms/step - loss: 0.2773 - accuracy: 0.8988 - val_loss: 0.3196 - val_accuracy: 0.8814\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 44s 70ms/step - loss: 0.2419 - accuracy: 0.9138 - val_loss: 0.3553 - val_accuracy: 0.8826\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 45s 71ms/step - loss: 0.2146 - accuracy: 0.9244 - val_loss: 0.3649 - val_accuracy: 0.8746\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.1847 - accuracy: 0.9348 - val_loss: 0.5931 - val_accuracy: 0.8250\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 45s 73ms/step - loss: 0.1741 - accuracy: 0.9403 - val_loss: 0.3745 - val_accuracy: 0.8730\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.1515 - accuracy: 0.9490 - val_loss: 0.3788 - val_accuracy: 0.8726\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.1326 - accuracy: 0.9541 - val_loss: 0.3839 - val_accuracy: 0.8436\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 46s 73ms/step - loss: 0.1204 - accuracy: 0.9597 - val_loss: 0.3910 - val_accuracy: 0.8706\n",
      "782/782 [==============================] - 33s 41ms/step - loss: 0.3369 - accuracy: 0.8730\n",
      "Test acc: 0.873\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, batch_size=batch_size, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "\n",
    "Model trains very slowly\n",
    "* inputs are large (600, 20,000). 12,000,000 floats per movie review\n",
    "\n",
    "Performs worse than the very light and fast binary unigram model\n",
    "\n",
    "One-hot not so good idea. Better: word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding word embeddings\n",
    "\n",
    "Word encoding: feature-engineering decision.\n",
    "\n",
    "Injections assumptions about the structure of the feature space\n",
    "\n",
    "One-hot assumption: The different tokens are all independent from each other (one-hot vectors are all orthogonal to one another)\n",
    "\n",
    "Example: \"film\" and \"movie\" encoded vectors\n",
    "\n",
    "\n",
    "geometric relationship <--> semantic relationship\n",
    "\n",
    "geometric distance (e.g. L2 distance)  <--> semantic distance\n",
    "\n",
    "Word embeddings : map human language into a structured geometric space.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-03.png\" width=\"150\"><p style=\"text-align:center\">Figure 11.2 A toy example \n",
    "of a word-embedding space.</p>\n",
    "\n",
    "One-hot encoding:\n",
    "* binary\n",
    "* sparse\n",
    "* very high-dimensional (20,000)\n",
    "\n",
    "Word embeddings:\n",
    "* floating-point\n",
    "* low-dimensional (256~1024)\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-02.png\" width=\"300\"><p style=\"text-align:center\">Figure 11.3 Word representations \n",
    "obtained from one-hot encoding or hashing are sparse, high-dimensional, \n",
    "and hardcoded. Word embeddings are dense, relatively low-dimensional, and \n",
    "learned from data.</p>\n",
    "\n",
    "Obtain word embeddings:\n",
    "* learn them jointly with the main task (start with random)\n",
    "* use pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning word embeddings with the Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating an `Embedding` layer**\n",
    "\n",
    "Word index $\\rightarrow$ Embedding layer $\\rightarrow$ Corresponding word vector\n",
    "\n",
    "Rank-2 (batch_size, sequence_length) $\\rightarrow$  Rank-3 (batch_size, sequence_length, embedding_dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model that uses an `Embedding` layer trained from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 256)         2560000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               55680     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,615,745\n",
      "Trainable params: 2,615,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 16s 22ms/step - loss: 0.5358 - accuracy: 0.7264 - val_loss: 0.5909 - val_accuracy: 0.7292\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.3386 - accuracy: 0.8702 - val_loss: 0.3327 - val_accuracy: 0.8598\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.2559 - accuracy: 0.9068 - val_loss: 0.3211 - val_accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.1997 - accuracy: 0.9287 - val_loss: 0.3344 - val_accuracy: 0.8776\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.1611 - accuracy: 0.9421 - val_loss: 0.4160 - val_accuracy: 0.8616\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.1238 - accuracy: 0.9575 - val_loss: 0.4064 - val_accuracy: 0.8766\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.0970 - accuracy: 0.9677 - val_loss: 0.4376 - val_accuracy: 0.8662\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.0739 - accuracy: 0.9758 - val_loss: 0.4680 - val_accuracy: 0.8662\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.0589 - accuracy: 0.9811 - val_loss: 0.5134 - val_accuracy: 0.8712\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 14s 22ms/step - loss: 0.0463 - accuracy: 0.9858 - val_loss: 0.6468 - val_accuracy: 0.8518\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3464 - accuracy: 0.8684\n",
      "Test acc: 0.868\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding padding and masking\n",
    "\n",
    "bidirectional RNN -> one RNN looks at the tokens in their natural order\n",
    "\n",
    "last iterations: vectors that encode padding (possibly for several hundreds of iterations)\n",
    "\n",
    "Masking: tell the RNN to skip padding (boolean vector)\n",
    "\n",
    "Example\n",
    "```\n",
    ">>> embedding_layer = Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    ">>> some_input = [\n",
    "... [4, 3, 2, 1, 0, 0, 0],\n",
    "... [5, 4, 3, 2, 1, 0, 0],\n",
    "... [2, 1, 0, 0, 0, 0, 0]]\n",
    ">>> mask = embedding_layer.compute_mask(some_input)\n",
    "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
    "array([[ True, True, True, True, False, False, False],\n",
    " [ True, True, True, True, True, False, False],\n",
    " [ True, True, False, False, False, False, False]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using an `Embedding` layer with masking enabled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 256)         2560000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,634,049\n",
      "Trainable params: 2,634,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 25s 33ms/step - loss: 0.4042 - accuracy: 0.8168 - val_loss: 0.2990 - val_accuracy: 0.8744\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2530 - accuracy: 0.9007 - val_loss: 0.3047 - val_accuracy: 0.8770\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.1939 - accuracy: 0.9280 - val_loss: 0.3072 - val_accuracy: 0.8732\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.1520 - accuracy: 0.9434 - val_loss: 0.3156 - val_accuracy: 0.8846\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.1149 - accuracy: 0.9595 - val_loss: 0.4068 - val_accuracy: 0.8636\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.0887 - accuracy: 0.9683 - val_loss: 0.4219 - val_accuracy: 0.8672\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.0640 - accuracy: 0.9778 - val_loss: 0.6186 - val_accuracy: 0.8484\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.0482 - accuracy: 0.9839 - val_loss: 0.5183 - val_accuracy: 0.8562\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.0373 - accuracy: 0.9879 - val_loss: 0.5327 - val_accuracy: 0.8590\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.5985 - val_accuracy: 0.8672\n",
      "782/782 [==============================] - 11s 12ms/step - loss: 0.3061 - accuracy: 0.8721\n",
      "Test acc: 0.872\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pretrained word embeddings\n",
    "\n",
    "사용 가능한 훈련 데이터가 너무 적은 경우 미리 계산된 임베딩을 사용할 수 있습니다.\n",
    "\n",
    "이미지 분류를 위해 사전 훈련된 convnet을 사용하는 것과 동일한 아이디어입니다.\n",
    "\n",
    "한 가지 예: 2014년 스탠포드 연구원들이 개발한 Global Vectors for Word Representation (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip # use browser instead.\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing the GloVe word-embeddings file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the GloVe word-embeddings matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model that uses a pretrained Embedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 100)         1000000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               34048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,034,113\n",
      "Trainable params: 34,113\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 25s 32ms/step - loss: 0.5834 - accuracy: 0.6809 - val_loss: 0.4660 - val_accuracy: 0.7890\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.4576 - accuracy: 0.7898 - val_loss: 0.4284 - val_accuracy: 0.8034\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.4082 - accuracy: 0.8180 - val_loss: 0.4189 - val_accuracy: 0.8014\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.3802 - accuracy: 0.8360 - val_loss: 0.3940 - val_accuracy: 0.8222\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.3588 - accuracy: 0.8484 - val_loss: 0.3599 - val_accuracy: 0.8424\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.3390 - accuracy: 0.8544 - val_loss: 0.3456 - val_accuracy: 0.8490\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.3213 - accuracy: 0.8645 - val_loss: 0.3597 - val_accuracy: 0.8498\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.3074 - accuracy: 0.8714 - val_loss: 0.3705 - val_accuracy: 0.8338\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2904 - accuracy: 0.8799 - val_loss: 0.3289 - val_accuracy: 0.8564\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.2788 - accuracy: 0.8855 - val_loss: 0.3232 - val_accuracy: 0.8614\n",
      "782/782 [==============================] - 11s 12ms/step - loss: 0.3103 - accuracy: 0.8675\n",
      "Test acc: 0.868\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary_1gram: 0.887\n",
    "* binary_2gram: 0.892\n",
    "* tfidf_2gram: 0.897\n",
    "* one_hot_bidir_lstm: 0.873\n",
    "* embeddings_bidir_lstm: 0.868\n",
    "* embeddings_bidir_lstm_with_masking: 0.872\n",
    "* glove_embeddings_sequence model: 0.868\n",
    "\n",
    "On this task pretrained embeddings were not very helpful\n",
    "\n",
    "Dataset contains enough samples to learn a specialized embedding space from scratch\n",
    "\n",
    "Works better for smaller datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "* Two kinds of NLP models:\n",
    " - bag-of-words models (no order) with Dense layers\n",
    " - sequence models that process word order with RNN, a 1D convnet, or a Transformer\n",
    " \n",
    "* Word embeddings: vector spaces where semantic relationships between words are modeled as distance relationships between vectors that represent those words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part01_introduction.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
